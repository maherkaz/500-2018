---
title: "Basic R Materials for 500"
author: "Thomas E. Love"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
geometry: margin=1in
fontsize: 12pt
---

```{r set-options, echo=FALSE, cache=FALSE, message=FALSE}
knitr::opts_chunk$set(comment=NA)
library(Hmisc); library(Epi); library(car)
```
---

> It is an odd feeling when you love what you do and everyone else seems to hate it. I get to peer into lists of numbers and tease out knowledge that can help people live longer, healthier lives. But if I tell friends I get a kick out of statistics, they inch away as if I have a communicable disease.

> -- Andrew Vickers *What is a P Value, Anyway?*

# Some Opening Thoughts

My goals in this document are to help catalyze your efforts towards ... 

1. Applying statistical methods in evaluating clinical or public health interventions without the use of a , emphasizing activities that might be plausible in a real research project
2. Using the R statistical programming language (free at cran.case.edu) and the R Studio interface (free at rstudio.com) and R Markdown to obtain statistical results for comparison and simple modeling given some data.

This material provides some insight into...

- Gathering, managing and describing data
- How to think about collecting some data
- How to get data into the infernal machine
- How to get some useful graphs/other stuff out of it
- How to fit multiple regression and logistic regression models in R

In fact, though, statistical thinking is about a lot more than this. At the very least, it's about

- planning the study, 
- collecting then cleaning the data, 
- analyzing the results, 
- interpreting the analyses and 
- presenting the study.

**Statistics is far too important to be left to statisticians!**

# Getting R and R Studio onto your computer

See the Notes section of our web site for some detailed instructions on getting R and R Studio onto your computer, as well as a separate document with some tips on using R Studio, and in particular, R Markdown.

# Getting Data into R from Excel or another Software Package: The Fundamentals

The easiest way to get data from another software package into R is to save the file (from within the other software package) in a form that R can read.  What you want is to end up with an Excel file that looks like this...

![sheet1.png](sheet1.png)

The variable names are in the first row, and the data are in the remaining rows (2-10 in this small example). Categorical variables are most easily indicated by letters (drug A or B, for instance) while continuous variables, like response, are indicated by numbers. Leave missing cells blank or use the symbol `NA`, rather than indicating them with, say, -99.

Within Excel, this file can be saved as a `.csv` (comma-separated text file) or just as an Excel .XLS file, and then imported directly into R, via RStudio by clicking Import Dataset under the Workspace tab, then selecting From Text File. If you've saved the file in Excel as a `.csv` file, RStudio will generally make correct guesses about how to import the file.  Once imported, you just need to save the workspace when you quit RStudio and you'll avoid the need to re-import.

# Describing a Diabetes Pilot Study

Consider the `dm401` data set, which provides (hypothetical) pilot demographic and clinical information for 146 continuity diabetic patients in a large metropolitan health system. The `dm401.csv` file's first ten observations are shown below.

![dm401-first10.png](dm401-first10.png)

```{r read in dm401 data set}
dm401 <- read.csv("dm401.csv")
head(dm401, 10) ## shows first ten observations
summary(dm401)
```

### A Bare Bones Data Dictionary

All measures are as of the date of study entry. We have:

- insurance `payer` in four categories
- level of hemoglobin `a1c`
- `ldl` cholesterol
- `sbp` is systolic blood pressure
- `pnvax` indicates a recorded pneumococcal vaccine at any time prior to study entry 
- `age` is in years
- `bmi` is body mass index
- `raceeth` is race/ethnicity in three categories
- `female` indicates gender
- `smoking` status (self-report of non-smoker or current smoker at study entry)
- `eyexm` indicates whether an eye examination is recorded in the past 12 months.

```{r structure of dm401}
str(dm401)
```

## Task 1: Cleaning the Data

We'll begin with some elementary cleaning. Is there any missingness in the data?  Do we have any unrealistic values in the data elements?  Do range checks pan out?

```{r describe dm401}
library(Hmisc)
describe(dm401)
```


## Task 2: Is there an important difference in BMI by gender?

I'll start here by re-creating the `bootdif` function, useful for building bootstrap confidence intervals for the population mean difference using independent samples.

```{r bootdif function}
`bootdif` <-
  function(y, g, conf.level=0.95, B.reps = 2000) {
    require(Hmisc)
    lowq = (1 - conf.level)/2
    g <- as.factor(g)
    a <- attr(smean.cl.boot(y[g==levels(g)[1]], B=B.reps, reps=TRUE),'reps')
    b <- attr(smean.cl.boot(y[g==levels(g)[2]], B=B.reps, reps=TRUE),'reps')
    meandif <- diff(tapply(y, g, mean, na.rm=TRUE))
    a.b <- quantile(b-a, c(lowq,1-lowq))
    res <- c(meandif, a.b)
    names(res) <- c('Mean Difference',lowq, 1-lowq)
    res
  }
```

```{r task2 plot dm401}
attach(dm401)
boxplot(bmi ~ female, ylab="Body Mass Index", main="Task 2, dm401 Example")
by(bmi, female, summary)
t.test(bmi ~ female)
bootdif(bmi, female)
```

## Task 3: Are the compliance measures (smoking status and eye exam) strongly correlated?

I'll start by re-creating the `twobytwo` function for performing detailed analyses of 2x2 tables.

```{r twobytwo function}
`twobytwo` <-
  function(a,b,c,d, namer1 = "Row1", namer2 = "Row2", namec1 = "Col1", namec2 = "Col2")
    # build 2 by 2 table and run Epi library's twoby2 command to summarize
    # from the row-by-row counts in a cross-tab
    # upper left cell is a, upper right is b, lower left is c, lower right is d
    # names are then given in order down the rows then across the columns
    # use standard epidemiological format - outcomes in columns, treatments in rows
  {
    require(Epi)
    .Table <- matrix(c(a, b, c, d), 2, 2, byrow=T, 
                     dimnames=list(c(namer1, namer2), c(namec1, namec2)))
    twoby2(.Table)
  }
```

```{r task 3 table dm401}
table(smoking, eyexm)
twobytwo(33, 73, 8, 32, "Non-Smoker", "Smoker", "Eye Exam", "No Eye Exam")
```

## Task 4: Is insurance status related to pneumovax?

```{r task4 table dm401}
table(insurance, pnvax)
chisq.test(table(insurance, pnvax))
```

## Task 5: Is systolic blood pressure related to age? Is this a linear relationship?

```{r task5 plot dm401}
plot(sbp~age, cex=1.7, main="Task 5, dm401 Example")
abline(lm(sbp ~ age), col="red")

summary(lm(sbp ~ age))
```

## Task 6: Is hemoglobin A1c linearly related to LDL cholesterol (treating A1c as the outcome?)

```{r task6 plot dm401}
plot(a1c ~ ldl, cex=1.7, main="Task 6, dm401 Example")
summary(lm(a1c ~ ldl))
```

## Task 7: What can we say about the relationships of insurance and race (separately and together) on A1c? Should we consider collapsing the smallest ``race/ethnicity'' category?

```{r task7 dm401}
summary(lm(a1c ~ insurance))
summary(lm(a1c ~ raceeth))
summary(lm(a1c ~ insurance + raceeth))

table(raceeth)
summary(lm(a1c ~ raceeth=="White"))
summary(lm(a1c ~ insurance + (raceeth=="White")))
summary(lm(a1c ~ insurance * (raceeth=="White")))
```

## Task 8: How does the impact of insurance (ignoring race/ethnicity) on A1c change if we adjust A1c for the effect of LDL?

```{r task8 dm401}
summary(lm(a1c ~ insurance))

summary(lm(a1c ~ insurance + ldl))
```

## Task 9: Build a kitchen sink model to predict A1c using main effects of the other ten variables as predictors. Then use the step function to identify a subset model for further analysis.

```{r task9 dm401}
summary(lm(a1c ~ ldl + sbp + insurance + eyexm + pnvax + age + bmi + raceeth + female + smoking))
step(lm(a1c ~ ldl + sbp + insurance + eyexm + pnvax + age + bmi + raceeth + female + smoking))
```

## Task 10: Does the smaller model produced by the stepwise analysis above look like a useful partition of the original set of predictors? Evaluate this by looking at significance tests, but also model summary statistics ($R^2$, RMSE, etc.) for each model.

```{r task10 dm401}
summary(lm(a1c ~ ldl + insurance + eyexm))
summary(lm(a1c ~ ldl + sbp + insurance + eyexm + pnvax + age + bmi + raceeth + female + smoking))

detach(dm401)
```

# The SEPSIS and Ibuprofen Study: A Logistic Regression Example

> This example is drawn from Dupont WD Statistical Modeling for Biomedical Researchers, Cambridge University Press, 2002: 1st Edition, Exercise 4.25. 
> The original study was Bernard GR et al. (1997) The effects of ibuprofen on the physiology and survival of patients with sepsis. The Ibuprofen in Sepsis Study Group. N Engl J Med 336: 912-918.

## The Data Set

We're going to look now at 30-day mortality in a sample of 350 septic patients as a function of 

- receiving either ibuprofen or placebo treatment, 
- their race (white or African-American), 
- and their baseline APACHE (Acute Physiology and Chronic Health Evaluation) score.

APACHE score is a composite measure of the patient's degree of morbidity collected just prior to recruitment into the study, and is highly correlated with survival. 
```{r read in sep data}
sep <- read.csv("sep.csv")
attach(sep)
summary(sep)
```

Note that `death30d` = 0 if patient was alive 30 days after study entry, 1 if patient was dead 30 days after study entry. 

We will estimate a **logistic regression model** to predict the probability of death at 30 days on the basis of these predictors. Overall, 39.14% were dead 30 days after study entry. 

## Is Death Rate related to APACHE scores?

```{r sep plot1}
boxplot(apache ~ death30d, horizontal=T, 
        names=c("Alive at 30d", "Dead at 30d"), xlab="APACHE Score at Baseline")

tapply(apache, death30d, summary)
```

It looks like higher APACHE scores (on average) are associated with 30-day mortality. Is this significant? Well, we could do a t test, or the regression equivalent, using APACHE as the outcome variable ...

```{r first try at a model for sep}
summary(lm(apache ~ death30d))
```

But that's backwards: death at 30 days is the *outcome* here, not a predictor. We need a regression model that predicts the probability of death! But, as we can see in the plot below, a straight line regression model won't predict `death30d` from `apache` well at all.

```{r sep plot2}
plot(death30d ~ apache, ylim=c(0,1.1), col="blue")
abline(lm(death30d ~ apache), col="red", lwd=2, lty=2)
```


## The Logistic Regression Model

We will develop a logistic regression model to predict prob(x) = the probability that a patient with apache score x will die. In logistic regression, we fit probability functions of the form $prob(x)=exp[a+bx]/(1+exp[a+bx])$, where a and b are unknown parameters (regression coefficients) that we will estimate from the data. So we have the logistic probability function

$$ prob(x) = \frac{exp[a+bx]}{1+exp[a+bx]} $$

This describes a family of curves appropriate for estimating probabilities on a 0-1 scale...

![logistic1.png](logistic1.png)

- The two solid curves (in blue and red) have the same value of the $b$ parameter, which gives identical slopes. 
- The different values of the $a$ parameter shift the red curve to the right of the blue curve. 
- The slopes of these curves increase as $b$ gets larger. 
- The magnitude of $b$ determined how quickly prob(x) rises from 0 to 1. 
- For a given $b$, $a$ controls where the 50% survival point is located. 
- Specifically, when $x = -a/b$, it turns out that prob(x) = 0.5, so, for instance, in our blue curve, prob(x) = 0.5 when x = 4/.4 = 10.

We can represent the probabilities in terms of their log odds, using the **logit function**:

$$ logit(prob(x)) = log \frac{(prob(x))}{(1-prob(x))} = a + bx $$

which works from any prob(x) between 0 and 1, where $a$ and $b$ are the regression coefficients for R to estimate, and the right-hand side is called the **linear predictor**. 

## Fitting a Logistic Regression Model

We wish to choose the best curve to fit our data. To do this, we inform R about our binary response variable (`death30d`, which is 1 for dead, 0 for alive), our predictor variable (`apache` score) and our desired regression function (the `logit`), as follows:

```{r sep fitting logistic model}
summary(glm(death30d ~ apache, family=binomial(logit)))
```

The logistic regression procedure estimates the two key parameters of the logistic probability function.

- Our intercept $a$ is estimated to be -2.27, and 
- Our slope $b$ for APACHE score is estimated to be 0.113, as can be seen in the coefficient estimates. 

So the fitted prediction model for the probability of death by 30 days based on APACHE score is...

$$ prob(x) = \frac{exp(a + bx)}{1 + exp(a + bx)} = \frac{exp(-2.27 + 0.113 apache)}{1 + exp(-2.27 + 0.113 apache)} $$

and we also know that the linear predictor is:

$$ logit(prob(x)) = log \frac{prob(x)}{1 - prob(x)} = a + bx = -2.27 + 0.113 apache. $$

## Using the Fitted Logistic Regression Model To Make Predictions

We have 350 observations in the `sep` data, and five variables.

```{r dimensions of sep}
dim(sep)
```

The first patient in the data set, shown below, had an APACHE score of 27.

```{r first patient}
sep[1,]
```

While we know that this patient died, based on their APACHE score and our model, what was their estimated probability of 30-day mortality?

- The linear predictor for patient 1 must be $-2.27 + 0.113 (27)$, or `r -2.27 + 0.113*27`.
- To get to a predicted probability, we'll need to exponentiate that result: 

$exp(-2.27 + 0.113 (27)) = exp(.781)$ or `r round(exp(-2.27 + 0.113*27),3)`

- And the logistic probabilty function yields:

$$prob(x) = \frac{exp(-2.27 + 0.113 apache)}{1 + exp(-2.27 + 0.113 apache)} = \frac{2.184}{1 + 2.184}$$ = `r round(2.184/3.184,2)` 

Similarly, the second patient has an APACHE score of 14. We can calculate their estimated 30-day mortality risk as follows:

- Linear predictor is -2.27 + 0.113 (14) = `r -2.27 + 0.113 * 14`
- Exponentiating, we get exp(-0.688) = `r round(exp(-2.27 + 0.113 * 14),4)`
- And so the probability of death by 30 days is 0.5026/(1 + 0.5026) = `r round(0.5026 / 1.5026, 2)`

The good news is that R will calculate these probabilities for you.

```{r fit sep model and make predictions}
model1 <- glm(death30d ~ apache, family=binomial(logit))
fitted(model1)[c(1:2)]
```

## Interpreting the Logistic Regression Model Summary

Returning to our fitted model, we are left to interpret the remaining logistic regression output.

```{r summary for model 1}
summary(model1)
```

We interpret the coefficients in terms of log odds, or (after exponentiating) as odds ratios.

- For instance, an increase of 1 point in APACHE score is associated with an increase of 0.113 in the log odds of 30-day mortality.
- Or, we can exponentiate the coefficient (i.e. calculate $exp[0.113] = 1.12$) which is interpreted as the odds ratio comparing the odds of death for a patient with APACHE score = x + 1 to the odds of death for a patient with APACHE score = x. 
- In general, $exp(x)$ is the odds ratio for the outcome (here, death) associated with a one-unit increase in x. 
- A property of logistic regression is that this ratio remains constant for all values of x. So in this case, an increase of one point in the APACHE score is associated with an increase by a factor of 1.12 in the odds of death.

Our *p* value is `2.39e-10` (or 2.39 x 10^{-10}, i.e. a very, very small number) for APACHE, indicating (according, technically, to a Wald test) that the APACHE score has statistically significant predictive value (at usual $\alpha$ levels) for 30-day mortality risk. 

- As in simple linear regression, our null hypothesis here is that the predictor is of no help in predicting the outcome, and our alternative is that the predictor is of statistically significant help. 
- Note that, as in simple linear regression, we generally don't interpret the *p* value associated with the intercept term, since we will by default include it in our logistic regression modeling.

## The Analysis of Deviance 

We'll skip the rest of the output here. To assess whether the model (overall) has a statistically significant effect, we can run an Analysis of Deviance table as follows (note that Anova must be capitalized here, and is part of the `car` library)...

```{r analysis of deviance for sep model1}
library(car)
model1 <- glm(death30d ~ apache, family=binomial(logit))
Anova(model1, type="II")
```

This table provides a *p* value for the improvement in the deviance statistic due to the inclusion of apache score in the model, and is in that sense somewhat comparable to an overall ANOVA F test in linear regression. Here, again, the impact is statistically significant.

# Logistic Regression with Multiple Predictors

Now, suppose we consider including additional information beyond the APACHE score, starting by including the treatment received by the patient. Does adding the treatment statistically significantly improve the quality of the predictions we make?

```{r model2 for sep}
summary(glm(death30d ~ apache + treatment, family=binomial(logit)))
```

It looks like the main effect of `treatment` doesn't add statistically significant predictive value (Wald test *p* = 0.248) to the model with APACHE score. What is we add `race` as well?

```{r model3 for sep}
model3 <- glm(death30d ~ apache + treatment + race, family=binomial(logit))
summary(model3)
```

## Making Predictions

We can calculate the fitted probabilities for the first two patients, using this model, as follows.

```{r fit probabilities from model3}
model3$fitted.values[1:2]
```

We can also calculate the linear predictors associated with the first two patients, using the following.

```{r linear predictors from model3}
model3$linear.predictors[1:2]

detach(sep)
```

# The `demodata` Example: A Data Management Primer

I built a small data set (100 rows, and 18 columns) contained in the `demodata.csv` file in the **Data and Code** page of the course website. The purpose is to demonstrate ways of importing data of varying types into R in ways that are useful for doing the sorts of analyses you'll do in your projects.

```{r read in demodata data set}
demodata <- read.csv("demodata.csv")
str(demodata)
```

## A Quick Summary of the Data, as Initially Imported

```{r summary 1}
summary(demodata) ## basic numerical summaries of the eighteen variables
```

# Recoding Continuous Variables, including Time-to-Event and Count Variables

Here are the first 10 rows of the first five variables in the `demodata.csv` file, as they appear in Excel.

![first5.png](first5.png)

Continuous variables are relatively easy to import into R. 

- The `age` variable has no missing values, while `test1`, `test2` and `test3` each contain various ways of representing missing values, indicated by `-999` for `test1`, by `NA` for `test2` and by blank cells (which R converts to NAs) for `test3`. 

When we import the demodata.csv file into R, we'll see from a summary of the first five columns in the data (those are the continuous variables here) that two of these approaches to coding missing data (`NA` and blanks) each work properly, while the use of `-999` causes problems.

After initial import into R, here's what the same part of the `demodata` data frame looks like...

```{r first five vars}
demodata[1:10, 1:5] ## shows first ten rows of the first five variables
summary(demodata[1:5]) ## summarizes the first five variables
```

In the `test2` and `test3` cases, we see that R correctly identifies the values `NA` (in the case of `test2`) and ``blank'' (in the case of `test3`) as indicating missingness. 

But, for `test1`, we have a problem, in that R thinks that the code value `-999` is in fact a legitimate value, rather than a placeholder indicating missingness, and includes those values of `-999` when calculating the minimum and other summary statistics. 

So, we need to fix test1 so that it treats the three -999s as missing values. To do this, try the following...

```{r fixing test1}
is.na(demodata$test1) <- demodata$test1==-999
summary(demodata$test1)
```

## Imputing Values for the Missing Observations in Continuous Variables

Here is one potential approach for imputing values for the missing observations in `test1`, `test2` and `test3`.

```{r na pattern in test1 and test2}
library(Hmisc)
na.pattern(demodata[c("test1", "test2")])
```

For test1 and test2, we have only 3 and 5 missing values, respectively, which is less than 10\% of the data, and less than 20 observations that are missing in each column. Confronted with relatively modest missingness like this, under certain circumstances, like in your class project, I might recommend a simple imputation before including these as covariates in a propensity model.

```{r imputing test1 and test2}
demodata$test1.i <- impute(demodata$test1, fun="random")
set.seed(500001); summary(demodata[c("test1", "test1.i")])

demodata$test2.i <- impute(demodata$test2, fun="random")
set.seed(500002); summary(demodata[c("test2", "test2.i")])
```

Note that I'm using the `set.seed()` function here just to guarantee that if I rerun this Markdown file, I'll get the same imputed values.

On the other hand, for `test3`, we have 57 missing out of 100 values in total. Since this is both more than 20 missing values, and more than 10\% of our data set, my project-specific advice indicates that we should create two new variables:

- one to indicate missingness in `test3`, which I will call `test3.NA` and
- another where we impute the same (I'll use the median) value for each missing observation in `test3`, which I'll call `test3.i` 

```{r imputing test3}
demodata$test3.NA <- as.numeric(is.na(demodata$test3))
demodata$test3.i <- impute(demodata$test3, fun=median)

summary(demodata[c("test3", "test3.i", "test3.NA")])
```

And we'd include `test1.i`, `test2.i` and both `test3.NA` and `test3.i` in our propensity model to represent the information, while leaving the original variables `test1`, `test2` and `test3` out of the model.

## Creating a Binary Variable from a Continuous one

One more type of recoding is creating a binary or multi-categorical variable from a continuous one. For instance, we might create a binary variable that divides our patients into two groups, based on whether they were above or below the age of, say, 50. Here, I'll make the arbitrary choice to put those with ages equal to 50 into the ``above'' group.

```{r grouping age50plus}
demodata$age.50plus <- as.numeric(demodata$age >= 50)
by(demodata$age, demodata$age.50plus, summary) ## sanity check on recoding
```

And we could create a factor, as well, from this new variable.

```{r factor for age50plus}
demodata$age.50plus.f <- 
  factor(demodata$age.50plus, levels=c(1,0), labels=c("50 plus", "Less than 50"))
table(demodata$age.50plus.f, demodata$age.50plus) ## another sanity check
```

## Creating A 4-Category Variable from a Continuous one

Now, what if we wanted to create a four-category factor by age? One approach would be to use the `cut2` function from the `Hmisc` library to select four groups of roughly equal size (these would be quartiles)...

```{r grouping age into 4 groups}
## assumes library(Hmisc) has already been run
demodata$age.4groups <- cut2(demodata$age, g=4)
by(demodata$age, demodata$age.4groups, summary) ## sanity check
```

Or, we could pre-specify that we want groups at Up to age 35, then 35 up to 50, and 50 up to 64 and finally 65 or older...

```{r grouping age into 4 pre-specified groups}
demodata$age.groups4 <- cut2(demodata$age, cuts=c(35,50,65))
by(demodata$age, demodata$age.groups4, summary)
```

By default, the results of applying the `cut2` function is a single factor that divides the subjects into groups.

# Recoding Binary Categorical Variables

Binary variables can come in many different forms. The easiest thing to deal with is a simple 1-0 numeric variable, where 1 indicates the presence of the characteristic and 0 its absence. But we can see lots of different options. 

![sheet2.png](sheet2.png)

- The `histA` variable has Yes and No values, `histB` has 1 for Yes and 2 for No, while `histC` is set up as we'd usually prefer. 
- Then variables `histD` and `histE` have missing values represented by `NA`s and blanks, respectively (which will work smoothly) 
- Yet `histF` has three kinds of missing values: `99` for missing, `88` for no response and `77` for ``don't know.'' We'll assume that all three possibilities should be treated as missing.

When we import the `demodata.csv` file into R, the `NA` and blanks approaches to coding missingness each work properly, but we still have work ahead.

```{r categorical vars}
summary(demodata[c("histA", "histB", "histC", "histD", "histE", "histF")])
```

## Creating Factors and 1-0 variables

Most of the time, we're going to want to create both a 1-0 (in standard epidemiological format) and a factor version of a binary variable. The 1-0 version is generally more useful for outcomes, exposures and covariates, but there are times when the factor version is also helpful. So, here's how I might do that.

### Converting `histA`

```{r histA}
table(demodata$histA)
```

For `histA`, we already have a factor variable (Yes/No), but we need to get that into standard epidemiological format (with presence [i.e. Yes] first, and absence [No] second) and I'll label that histA.f, and then we'll also want a 1-0 numeric version, which I'll call `histA`, after I copy the original data to `histA.original`.

```{r histA conversions}
demodata$histA.original <- demodata$histA
demodata$histA.f <- factor(demodata$histA, levels=c("Yes","No"))
demodata$histA <- as.numeric(demodata$histA.f == "Yes")

table(demodata$histA, demodata$histA.f)
table(demodata$histA.original, demodata$histA.f)
summary(demodata[c("histA.original", "histA", "histA.f")])
```

### Converting `histB`

```{r histB}
table(demodata$histB)
```

For `histB`, we already have a numeric variable, where 1 = Yes, and 2 = No, but we need to get that into 1-0 form, and also build a factor to describe the results in standard epidemiological format. To do so, use the following:

```{r histB conversions}
demodata$histB.original <- demodata$histB
demodata$histB <- as.numeric(demodata$histB == 1)
demodata$histB.f <- factor(demodata$histB, levels=c(1,0), labels=c("Yes", "No"))

table(demodata$histB, demodata$histB.original)
table(demodata$histB, demodata$histB.f)
summary(demodata[c("histB.original", "histB", "histB.f")])
```

### Converting `histC`

```{r histC} 
table(demodata$histC)
```

For `histC`, we already have a numeric variable, where 1 = Yes, and 0 = No, so that's great, and all we need is to also build a factor to describe the results in standard epidemiological format. To do so, use the following:

```{r histC conversion}
demodata$histC.f <- factor(demodata$histC, levels=c(1,0), labels=c("Yes", "No"))
table(demodata$histC, demodata$histC.f)
```

OK, this looks great. 

## Dealing with Missingness in Binary Data

Now, we'll deal with missingness, in binary data, as shown in `histD`, `histE` and `histF`.

### Imputation for `histD`

```{r histD}
table(demodata$histD, useNA="ifany")
```

In `histD`, we have a 1-0 numeric variable, and have successfully gotten R to recognize 6 missing values. To use this as a covariate, we'll first impute (simply) the 6 missing values, since we have less than 20 missing values (and less than 10% of our data missing, for that matter.)

```{r histD imputation}
## set seed to ensure that imputations are the same if we rerun the file
set.seed(500003); demodata$histD.i <- impute(demodata$histD, fun="random")
demodata$histD.f <- factor(demodata$histD.i, levels=c(1,0), labels=c("Yes","No"))
## And we'll do some sanity checks, as usual.
summary(demodata[c("histD", "histD.i", "histD.f")])
```

### Working with `histE`

```{r histE}
table(demodata$histE, useNA="ifany")
```

In `histE`, we again have a 1-0 numeric variable, and R has recognized 27 missing values. To use this as a covariate, we'll create both an indicator of missingness (called `histE.NA`) and then do a simple imputation of the same value for each of the 27 missing values, putting the result in `histE.i`. Then, we'll create a factor called `histE.f` with three levels: Yes, No and Missing. 

```{r histE imputation and conversions}
demodata$histE.NA <- as.numeric(is.na(demodata$histE))
demodata$histE.i <- impute(demodata$histE, fun=median)
demodata$histE.f <- factor(demodata$histE, levels=c(1,0), labels=c("Yes","No"), exclude=NULL)
## The exclude=NULL part keeps in the NAs. 
## And we move on to sanity checking ...
summary(demodata[c("histE", "histE.i", "histE.NA", "histE.f")])
```

### Working with `histF`

```{r histF}
table(demodata$histF, useNA="ifany")
```

In `histF`, we again have a 1-0 numeric variable, but now we have codes `77`, `88` and `99`, all of which we'll take to mean missing values. So, we'll get R to recognize these values as missing in a new version of `histF`. Then, to use this as a covariate, we'll do a simple imputation (since the missingness rate < 10% and there are less than 20 missing values) into a variable called `histF.i`. Then, we'll create a factor called `histF.f` with two levels: Yes and No, based on the imputed values in `histF.i`.

```{r histF imputation and conversions}
demodata$histF.original <- demodata$histF
is.na(demodata$histF) <- demodata$histF > 1
table(demodata$histF, useNA="ifany")

set.seed(500004); demodata$histF.i <- impute(demodata$histF, fun="random")
demodata$histF.f <- factor(demodata$histF.i, levels=c(1,0), labels=c("Yes","No"))
summary(demodata[c("histF.original", "histF", "histF.i", "histF.f")])
```

# Recoding Categorical Variables with More Than Two Categories

There are lots of things we might want to do with a multi-categorical variable, including rearranging its levels, create factors which are labeled properly and appear in a sensible order, create binary 1/0 variables for individual categories, deal with missingness sensibly, and collapse categories. In addition, a multi-categorical variable can be coded originally in several different forms. 

![sheet3.png](sheet3.png)

We have five such variables here.

- `race` is coded as 1 = White, 2 = Black, 3 = Asian and 4 = All Other, with no missing values
- `rating` is either Exc, V Good, Good, Fair or Poor. There are 4 missing values, coded by `NA`.
- `return` is either A, B, C, or D. There are 26 missing values, coded in the .csv file by blanks.
- `rotation` is either X, Y or Z. There are 4 missing values, coded in the .csv as ``Unknown''.
- `reason` can take on 12 different values for primary reason why the subject did not go to the doctor. The `reason` variable has no missing values, but we might want to collapse the reasons into three groups, perhaps combining the several reasons pertaining to fear into one category, the reasons related to cost into another category, and reasons related to time into a third category.

```{r multi-categorical variables}
summary(demodata[c("race", "rating", "return", "rotation", "reason")])

table(demodata$reason, useNA="ifany")
```

## Working with `race`

As mentioned, `race` is coded as 1 = White, 2 = Black, 3 = Asian and 4 = All Other, with no missing values

```{r race}
table(demodata$race, useNA="ifany")
```

To use race as a covariate, we would want to create a factor, as follows.

```{r race as a factor}
demodata$race.f <- 
  factor(demodata$race, levels=c(1, 2, 3, 4), 
         labels=c("White", "Black", "Asian", "Other"))
table(demodata$race, demodata$race.f, useNA="ifany")
```

Also, we would likely need a series of indicator / dummy 1-0 numeric variables, one for each of the four categories of race, although we might only use three of them in modeling.

```{r race as indicator variables}
demodata$race.White <- as.numeric(demodata$race.f=="White")
demodata$race.Black <- as.numeric(demodata$race.f=="Black")
demodata$race.Asian <- as.numeric(demodata$race.f=="Asian")
demodata$race.Other <- as.numeric(demodata$race.f=="Other")

## Some quick sanity checks
summary(demodata[c("race", "race.f", "race.White")])
table(demodata$race.f, demodata$race.Asian)
```

## Working with `rating`

`rating` is either Exc, V Good, Good, Fair or Poor. There are 4 missing values, coded by `NA`.

```{r rating}
table(demodata$rating, useNA="ifany")
```

That is a factor, but an annoyingly poor ordering of the variables. We could adjust that...

```{r rating rearrangement}
demodata$rating.f <- factor(demodata$rating, levels=c("Exc", "V Good", "Good", "Fair", "Poor"), exclude=NULL)
table(demodata$rating, demodata$rating.f, useNA="ifany")
```

That's a much more meaningful ordering, but we still have four missing values. We could either impute (probably the better choice for your project) or create a new category for Missingness. Given that there are only 4 missing values (much less than 20) I would just impute, simply, as follows...

```{r rating imputation}
set.seed(500005); demodata$rating.f.i <- impute(demodata$rating.f, fun="random")
table(demodata$rating.f, demodata$rating.f.i, useNA="ifany")
```

And, as before, we could then create a series of indicator variables to represent the various categories.

What if we wanted to compare those with Exc, V Good or Good results to those with Fair or Poor results, in a binary variable? To do that, we could use the following approach:

```{r ratings binarized}
demodata$rating.10 <- as.numeric(demodata$rating.f=="Exc" | 
                                   demodata$rating.f.i=="V Good" | 
                                   demodata$rating.f.i=="Good")
table(demodata$rating.f.i, demodata$rating.10)
```

## Working with `return`

`return` is either A, B, C, or D. There are 26 missing values, coded in the `.csv` file by blanks.

```{r return}
table(demodata$return, useNA="ifany")
```

The blanks don't come through here as missing values, but instead look like another category called "blank." While that might work if we want to create a new category to deal with missingness, we probably want to first convert the variable so R recognizes the missingness.

```{r return missingness}
demodata$return.original <- demodata$return
is.na(demodata$return) <- demodata$return==""
demodata$return.f <- factor(demodata$return, 
                            levels=c("A", "B", "C", "D"), exclude=NULL)
table(demodata$return.f, useNA="ifany")

demodata$return.f.i <- impute(demodata$return.f, "Missing")
table(demodata$return.f.i)
```

Again, we could then create a series of indicator variables to represent the various categories, should we want them.

## Working with `rotation`

`rotation` is either X, Y or Z. There are 4 missing values, coded in the `.csv` as `"Unknown"`.

```{r rotation}
table(demodata$rotation, useNA="ifany")
is.na(demodata$rotation) <- demodata$rotation=="Unknown"

demodata$rotation.f <- factor(demodata$rotation, levels=c("X", "Y", "Z"), exclude=NULL)
demodata$rotation.f.i <- impute(demodata$rotation.f, fun="random")

table(demodata$rotation.f, demodata$rotation.f.i, useNA="ifany")
```

Once again, we could create indicator variables to represent the various categories, should we want them.

## Working with `reason`

`reason` can take on 12 different values for primary reason why the subject did not go to the doctor. 

```{r reason}
table(demodata$reason, useNA="ifany")
```

The `reason` variable has no missing values, but we might want to collapse the reasons into three groups, perhaps combining the several reasons pertaining to fear into one category, the reasons related to cost into another category, and reasons related to time into a third category.

Suppose your desired combination was as follows:

Old Reason (12 categories)          | New Reason (3 categories)
----------------------------------: | :-------------------------:
anxiety, fear, panic, unease, worry | fear
costly, expensive, high priced      | cost
no time, swamped, tied up, too busy | time

So, we'll build a new factor that includes only our three new categories. This is a little tricky: first we create a new variable with no data in it, but only including the three new categories.

```{r revising reason into 3 categories - a}
demodata$reason3.f <- factor(rep(NA, length(demodata$reason) ), levels=c("fear", "cost", "time"))
table(demodata$reason3.f, useNA="ifany")
```

Next, we fill in the `fear` values, followed by the `cost` and then `time` values until our variable is completed, with no remaining `NA`s.

```{r revising reason into 3 categories - b}
demodata$reason3.f[demodata$reason %in% c("anxiety", "fear", "panic", "unease", "worry")] <- "fear"
table(demodata$reason3.f, useNA="ifany")

demodata$reason3.f[demodata$reason %in% c("costly", "expensive", "high priced")] <- "cost"
demodata$reason3.f[demodata$reason %in% c("no time", "swamped", "tied up", "too busy")] <- "time"
table(demodata$reason3.f, useNA="ifany")
```

# Date Variables

If you've got a `.csv` file that was built in Excel, there are three likely data formats for dates that you'll see, as demonstrated in the `date1` and `date2` variables. 

![sheet4.png](sheet4.png)

Neither import well into R. `date1` produces an unordered factor, and `date2` just produces a set of integers.

```{r dates}
str(demodata[c("date1", "date2")])
```

## The `date` format in Excel yields `date1`

The `date1` approach is obtained using the date format in Excel, and is fine for humans to read, even in R, but R still has no idea how to use it, interpreting it as a factor. The data are provided in month/day/4-digit year format. In order to get R to treat this as a date, we use the following...

```{r reading in date1}
demodata$date1.fix <- as.Date(demodata$date1, "%m/%d/%Y")
```

The command includes a capital Y since the data include all 4 digits of the year. 

```{r date1 revised}
str(demodata$date1.fix)
summary(demodata$date1.fix)
```

## The `general` format in Excel yields `date2`

For `date2`, which contains **exactly** the same data as `date1`, but using the general format in Excel, R just sees an integer. But what Excel is actually trying to represent is ``days since 12/31/1899'' so that 1 = January 1, 1900. This isn't too useful for a computer or a human, although you can at least calculate differences between two dates in terms of number of days with such an approach. Another problem is that Excel's function for doing this believes that 1900 was a leap year. So, to account for this, we use the following approach to build a date.

```{r reading in date2}
demodata$date2.fix <- as.Date(demodata$date2, origin="1899-12-30")
str(demodata$date2.fix)
summary(demodata$date2.fix)
```



