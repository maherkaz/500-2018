---
title: "The Toy Example - A Demo for 500"
author: "Thomas E. Love, Ph.D."
date: 'Version: `r Sys.Date()`'
output:
  pdf_document:
    toc: yes
    number_sections: yes
  html_document:
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA)
```

## Setup: Loading Packages

```{r load data and packages, message = FALSE}
library(pander); library(tableone); library(broom)
library(Epi); library(survival); library(arm)
library(Hmisc); library(Matching); library(lme4)
library(twang); library(survey); library(rbounds)
library(cobalt); library(tidyverse)
```

# The Toy Data at Load-In {.tabset}

This document is about showing a (relatively) simple way to do things. In no way would
I claim that the approaches provided here are optimal. This is just a first demonstration.

## The Data Set

The Data Set is 100\% fictional, and is available as toy.csv on the course website. It contains data on 200 subjects (70 treated - subjects 131-200 and 130 controls - subjects 1-130) on treatment status, six covariates, and three outcomes, with no missing observations anywhere. We assume that a logical argument suggests that the square of `covA`, as well as the interactions of `covB` with `covC` and with `covD` should be related to treatment assignment, and thus should be included in our propensity model.

Our objective is to estimate the average causal effect of treatment (as compared to control) on each of the three outcomes, without propensity adjustment, and then with propensity matching, subclassification, weighting and regression adjustment using the propensity score.

```{r}
toy <- read_csv("toy.csv")
toy$out2.event <- factor(toy$out2.event)
toy$covF <- factor(toy$covF)
toy
```

## The Codebook

```{r, echo = FALSE}
codebook <- data_frame(
    Variable = c("subject", "treated", "covA", "covB", "covC", "covD", "covE", "covF", "out1.cost", "out2.event", "out3.time"),
    Type = c("Subject ID", "2-level categorical (0/1)", "Quantitative (2 decimal places)",
                         "2-level categorical (0/1)", "Quantitative (1 decimal place)",
                         "Quantitative (1 decimal place)", "Integer",
                         "3-level ordinal factor", "Quantitative outcome",
                         "Binary outcome (did event occur?)", "Time to event outcome"),
    Notes = c("1-130 = controls, 131-200 = treated", "0 = control, 1 = treated", 
              "reasonable values range from 0 to 1", "0 = no, 1 = yes",
              "range 3-20", "range 3-20", "range 3-20", "1 = Low, 2 = Middle, 3 = High",
              "typical values 20-80", "Yes/No (note: event is bad)", 
              "Time before event is observed or subject exits study (censored), range is 75-156 weeks")
)

pander(codebook)
```

With regard to the `out3.time` variable, subjects with `out2.event` = No were censored, so that `out2.event` = Yes indicates an observed event.

## Numerical Summaries

```{r}
summary(toy)
```

## Table 1 

```{r}
varlist = c("covA", "covB", "covC", "covD", "covE", "covF", 
            "out1.cost", "out2.event", "out3.time")
factorlist = c("covB", "covF", "out2.event")
CreateTableOne(vars = varlist, strata = "treated", 
               data = toy, factorVars = factorlist)
```

# Data Management and Cleanup

## Range Checks for Quantitative (continuous) Variables

Checking and cleaning the quantitative variables is pretty straightforward - the main thing I'll do at this stage is check the ranges of values shown to ensure that they match up with what I'm expecting. Here, all of the quantitative variables have values that fall within the "permissible" range described by my codebook, so we'll assume that for the moment, we're OK on `subject` (just a meaningless code, really), `covA`, `covC`, `covD`, `covE`, `out1.cost` and `out3.time`, and we see no missingness.

## Restating Categorical Information in Helpful Ways

The cleanup of the toy data focuses, as it usually does, on variables that contain **categories** of information, rather than simple counts or measures, represented in quantitative variables. 

### Re-expressing Binary Variables as Numbers and Factors

We have three binary variables (`treated`, `covB` and `out2.event`). A major issue in developing these variables is to ensure that the direction of resulting odds ratios and risk differences are consistent and that cross-tabulations are in standard epidemiological format. 

It will be useful to define binary variables in two ways: 

- as a numeric indicator variable taking on the values 0 (meaning "not having the characteristic being studied") or 1 (meaning "having the characteristic being studied") 
- as a text factor - with the levels of our key exposure and outcomes arranged so that "having the characteristic" precedes "not having the characteristic" in R when you create a table, but the covariates should still be No/Yes.

So what do we currently have? From the output below, it looks like `treated` and `covB` are numeric, 0/1 variables, while `out2.event` is a factor with levels "No" and then "Yes"

```{r}
toy %>% select(treated, covB, out2.event) %>% summary()
```

So, we'll create factors for `treated` and `covB`:

```{r}
toy$treated_f <- factor(toy$treated, levels = c(1,0), labels = c("Treated", "Control"))
toy$covB_f <- factor(toy$covB, levels = c(0,1), labels = c("No B", "Has B"))
```

For `out2.event`, on the other hand, we don't have either quite the way we might want it. As you see in the summary output, we have two codes for `out2.event` - either No or Yes, in that order. But we want Yes to precede No (and I'd like a more meaningful name). So I redefine the factor variable, as follows.

```{r}
toy$out2_f <- factor(toy$out2.event, levels = c("Yes","No"), labels = c("Event","No Event"))
```

To obtain a numerical (0 or 1) version of `out2.event` we can use R's `as.numeric` function - the problem is that this produces values of 1 (for No) and 2 (for Yes), rather than 0 and 1. So, I simply subtract 1 from the result, and we get what we need.

```{r}
toy$out2 <- as.numeric(toy$out2.event) - 1
```

### Testing Your Code - Sanity Checks

Before I move on, I'll do a series of sanity checks to make sure that our new variables are defined as we want them, by producing a series of small tables comparing the new variables to those originally included in the data set.

```{r}
table(toy$treated_f, toy$treated)
table(toy$covB_f, toy$covB)
table(toy$out2_f, toy$out2.event)
table(toy$out2, toy$out2.event)
table(toy$out2, toy$out2_f)
```

Everything looks OK:

- `treated_f` correctly captures the information in `treated`, with the label Treated above the label Control in the rows of the table, facilitating standard epidemiological format.
- `covB_f` also correctly captures the `covB` information, placing "Has B" last.
- `out2_f` correctly captures and re-orders the labels from the original `out2.event`
- `out2` shows the data correctly (as compared to the original `out2.event`) with 0-1 coding.

## Dealing with Variables including More than Two Categories

When we have a multi-categorical (more than two categories) variable, like `covF`, we will want to have

- both a text version of the variable with sensibly ordered levels, as a factor in R, as well as 
- a series of numeric indicator variables (taking the values 0 or 1) for the individual levels.

```{r}
summary(toy$covF)
```

From the `summary` output, we can see that we're all set for the text version of `covF`, as what we have currently is a factor with three levels, labeled 1-Low, 2-Middle and 3-High. This list of variables should work out well for us, as it preserves the ordering in a table and permits us to see the names, too. If we'd used just Low, Middle and High, then when R sorted a table into alphabetical order, we'd have High, then Low, then Middle - not ideal.

### A Brief Digression

Suppose, for the moment, that a different categorical variable had been included in our data set - this one, which we'll call `cat4`, has four levels, called (in the imported data: 1, 2, 3 and 4) - I'd turn this into a factor using this command:

```
cat4.f <- factor(cat4, levels=c(1,2,3,4), labels=c("Group 1", "Group 2", "Group 3", "Group 4"))
```

or, perhaps, instead, something like this:

```
cat4.f <- factor(cat4, levels=c(1,2,3,4), labels=c("1-Lowest", "2-Low", "3-High", "4-Highest"))
```

### Preparing Indicator Vraiables for `covF`

So, all we need to do for `covF` is prepare indicator variables. We can either do this for all levels, or select one as the baseline, and do the rest. Here, I'll show them all.

```{r}
## Re-expressing the Multi-Categorical Variable
toy$covF.Low <- as.numeric(toy$covF=="1-Low")
toy$covF.Middle <- as.numeric(toy$covF=="2-Middle")
toy$covF.High <- as.numeric(toy$covF=="3-High")
```

And now, some more sanity checks for the `covF` information:

```{r}
table(toy$covF, toy$covF.Low)
table(toy$covF, toy$covF.Middle)
table(toy$covF, toy$covF.High)
```

## Creating the Transformation and Product Terms

Remember that we have reason to believe that the square of `covA` as well as the interaction of `covB` with `covC` and also `covB` with `covD` will have an impact on treatment assignment. It will be useful to have these transformations in our data set for modeling and summarizing. I will use `covB` in its numeric (0,1) form (rather than as a factor - `covB.f`) when creating product terms, as shown below.

```{r}
toy$Asqr <- toy$covA^2
toy$BC <- toy$covB*toy$covC
toy$BD <- toy$covB*toy$covD
```

# Data Set After Cleaning {.tabset}

## Glimpse

```{r}
glimpse(toy)
```

## Summary

```{r}
summary(toy)
```

## Table 1

Note that the factors I created for the `out2` outcome are not well ordered for a Table 1, but are well ordered for other tables we'll fit later. So, in this case, I'll use the numeric version of the `out2` outcome, but the new factor representations of `covB` and `treated`.

```{r}
varlist = c("covA", "covB_f", "covC", "covD", "covE", "covF", 
            "out1.cost", "out2", "out3.time")
factorlist = c("covB_f", "covF", "out2")
CreateTableOne(vars = varlist, strata = "treated_f", 
               data = toy, factorVars = factorlist)
```


# The 13 Tasks We'll Tackle in this Example

1. Ignoring the covariate information, what is the unadjusted point estimate (and 95\% confidence interval) for the effect of the treatment on each of the three outcomes (`out1.cost`, `out2.event`, and `out3.time`)?
2. Assume that theory suggests that the square of `covA`, as well as the interactions of `covB` with `covC` and `covB` with `covD` should be related to treatment assignment. Fit a propensity score model to the data, using the six covariates (A-F) and the three transformations (A^2^, and the B-C and B-D interactions.) Plot the resulting propensity scores, by treatment group, in an attractive and useful way.
3. Use Rubin's Rules to assess the overlap of the propensity scores and the individual covariates prior to the use of any propensity score adjustments.
4. Use 1:1 greedy matching to match all 70 treated subjects  to control subjects without replacement on the basis of the linear propensity for treatment. Evaluate the degree of covariate imbalance before and after propensity matching for each of the six covariates, and present the pre- and post-match standardized differences and variance ratios for the covariates, as well as the square term and interactions, as well as both the raw and linear propensity score in appropriate plots. Now, build a new data frame containing the propensity-matched sample, and use it to first check Rubin's Rules after matching.
5. Now, use the matched sample data set to evaluate the treatment's average causal effect on each of the three outcomes. In each case, specify a point estimate (and associated 95\% confidence interval) for the effect of being treated (as compared to being a control subject) on the outcome. Compare your results to the automatic versions reported by the Matching package when you include the outcome in the matching process.
6. Now, instead of matching, instead subclassify the subjects into quintiles by the raw propensity score. Display the balance in terms of standardized differences by quintile for the covariates, their transformations, and the propensity score in an appropriate table or plot(s). Are you satisfied? 
7. Regardless of your answer to the previous question, use the propensity score quintile subclassification approach to find a point estimate (and 95% confidence interval) for the effect of the treatment on each outcome. 
8. Now using a reasonable propensity score weighting strategy, assess the balance of each covariate, the transformations and the linear propensity score prior to and after propensity weighting. Is the balance after weighting satisfactory?
9. Using propensity score weighting to evaluate the treatment's effect, developing a point estimate and 95% CI for the average causal effect of treatment on each outcome.
10. Finally, use direct adjustment for the linear propensity score on the entire sample to evaluate the treatment's effect, developing a point estimate and 95\% CI for each outcome.
11. Now, try a double robust approach. Weight, then adjust for linear propensity score.
12. Compare your conclusions about the average causal effect obtained in the following six ways to each other. What happens and why? Which of these methods seems most appropriate given the available information?
    + without propensity adjustment, 
    + after propensity matching, 
    + after propensity score subclassification, 
    + after propensity score weighting, 
    + after adjusting for the propensity score directly, and 
    + after weighting then adjusting for the PS, to each other.  
13. Perform a sensitivity analysis for your matched samples analysis and the first outcome (`out1.cost`) if it turns out to show a statistically significant treatment effect.

# Task 1. Ignoring covariates, estimate the effect of treatment vs. control on...

## Outcome 1 (a continuous outcome)

Our first outcome describes a quantitative measure, cost, and we're asking what the effect of `treatment` as compared to `control` is on that outcome. Starting with brief numerical summaries:

```{r}
toy %>%
    group_by(treated_f) %>%
    summarize(n = length(out1.cost), mean = mean(out1.cost), 
              sd = sd(out1.cost), min = min(out1.cost), 
              Q1 = quantile(out1.cost, 0.25), median = median(out1.cost), 
              Q3 = quantile(out1.cost, 0.75), max = max(out1.cost))
```

It looks like the Treated group has higher costs than the Control group. To model this, we could use a linear regression model to obtain a point estimate and 95% confidence interval. Here, I prefer to use the numeric version of the `treated` variable, with 0 = "control" and 1 = "treated".

```{r}
unadj.out1 <- lm(out1.cost ~ treated, data=toy)
summary(unadj.out1); confint(unadj.out1, level = 0.95) ## provides treated effect and CI estimates
```

We can store these results in a data frame, with the `tidy` function from the `broom` package.

```{r}
temp <- tidy(unadj.out1, conf.int = TRUE, conf.level = 0.95)
temp
```

Our unadjusted treatment effect estimate is an increase of `r temp %>% filter(term == "treated") %>% select(estimate) %>% round(digits = 2)` in cost, with 95\% confidence interval (`r round(confint(unadj.out1)["treated",],2)`).

I should mention that the `broom` package also has a useful function called `glance` which lets you get some detailed summaries of the model.

```{r}
glance(unadj.out1)
```

## Outcome 2 (a binary outcome)

### Using a 2x2 table in standard epidemiological format

Thanks to our preliminary cleanup, it's relatively easy to obtain a table in standard epidemiological format comparing treated to control subjects in terms of `out2`:

```{r}
table(toy$treated_f, toy$out2_f)
```

Note that the exposure is in the rows, with "Having the Exposure" or "Treated" at the top, and the outcome is in the columns, with "Yes" or "Outcome Occurred" or "Event Occurred" on the left, so that the top left cell count describes people that had both the exposure and the outcome. That's *standard epidemiological format*, just what we need for the `twoby2` function in the `Epi` package.

```{r}
twoby2(table(toy$treated_f, toy$out2_f))
```

Eventually, we will be interested in at least two measures - the odds ratio and the risk (probability) difference estimates, and their respective confidence intervals.

- For a *difference in risk*, our unadjusted treatment effect estimate is an increase of 11.1 percentage points as compared to control, with 95\% CI of (-3.4, +24.9) percentage points.
- For an *odds ratio*, our unadjusted treatment effect estimate is an odds ratio of 1.56 (95\% CI = 0.87, 2.80) for the event occurring with treatment as compared to control. 
- For neither measure is the observed unadjusted effect statistically significant at a 95\% confidence level.

### Using a logistic regression model

For the odds ratio estimate, we can use a simple logistic regression model to estimate the unadjusted treatment effect, resulting in essentially the same answer. We'll use the numerical (0/1) format to represent binary information, as follows.

```{r}
unadj.out2 <- glm(out2 ~ treated, data=toy, family=binomial())
summary(unadj.out2)
exp(coef(unadj.out2)) # produces odds ratio estimate
exp(confint(unadj.out2)) # produces 95% CI for odds ratio
```

And, again, we can use the `tidy` function in the `broom` package to build a tibble of the key parts of the output. Note that by including the `exponentiate = TRUE` command, our results in the `treated` row describe the odds ratio, rather than the log odds.

```{r}
tidy(unadj.out2, conf.int = TRUE, exponentiate = TRUE)
```

- Our odds ratio estimate remains about 1.56, with 95% confidence interval ranging from 0.9 to 2.8, again showing no evidence of a statistically significant impact of the treatment on the occurrence of the event described by `out2`.
- For practical purposes, the odds ratio and 95\% confidence interval obtained here matches the methodology for the `twoby2` function. 
- The approach implemented in the `twoby2` function produces slightly less conservative (i.e. narrower) confidence intervals for the effect estimate than does the approach used in the logistic regression model.

Glancing at the summaries of the model may be helpful in some settings, as well.

```{r}
glance(unadj.out2)
```

## Outcome 3 (a time-to-event outcome with right censoring)

Our `out3.time` variable is a variable indicating the time before the event described in `out2` occurred. This happened to 97 of the 200 subjects in the data set. For the other 103 subjects who left the study before their event occurred, we have the time before censoring. We can see the results of this censoring in the survival object describing each treatment group. 

Here, for instance, is the survival object for the *treated* subjects - the third subject listed here is censored - had the event at some point after 124 weeks (124+) but we don't know precisely when after 124 weeks.

```{r}
Surv(toy$out3.time, toy$out2.event == "Yes")[toy$treated == 1]
```

- To see the controls, we could use `Surv(toy$out3.time, toy$out2.event=="Yes")[toy$treated==0]`

To deal with the right censoring, we'll use the `survival` package to fit a simple unadjusted Cox proportional hazards model to assess the relative hazard of having the event at a particular time point among treated subjects as compared to controls.

```{r}
unadj.out3 <- coxph(Surv(out3.time, out2.event=="Yes") ~ treated, data=toy)
summary(unadj.out3) ## exp(coef) section indicates relative risk estimate and 95% CI
```

The relative hazard rate is shown in the `exp(coef)` section of the output. Our unadjusted treatment model suggests that the hazard of the outcome is smaller (but not significantly smaller) in the treated group than in the control group. Our estimate is that this relative hazard rate for occurrence of the event associated with treatment as compared to control is 0.86 with a 95% confidence interval of (0.57, 1.29).

Yes, you can tidy this model, as well, using the `broom` package.

```{r}
tidy(unadj.out3, exponentiate = TRUE)
```

Glancing at the summaries of the model may be helpful, too.

```{r}
glance(unadj.out3)
```


It's wise, whenever fitting a Cox proportional hazards model, to assess the proportional hazards assumption. One way to do this is to run a simple test in R - from which we can obtain a plot, if we like. The idea is for the plot to show no clear patterns over time, and look pretty much like a horizontal line, while we would like the test to be non-significant - if that's the case, our proportional hazards assumption is likely OK.

```{r}
cox.zph(unadj.out3)
plot(cox.zph(unadj.out3), var="treated")
```

If the proportional hazards assumption is clearly violated (here it isn't), call a statistician.

## Unadjusted Estimates of Treatment Effect on Outcomes

So, our unadjusted average treatment effect estimates (in each case comparing treated subjects to control subjects) are thus:

Est. Treatment Effect (95\% CI) | Outcome 1 (Cost diff.) | Outcome 2 (Risk diff.) | Outcome 2 (Odds Ratio) | Outcome 3 (Relative Hazard Rate)
----------------: | -----------: | -----------: | -----------: | -----------: 
No covariate adjustment | **15.7** | **+0.11** | **1.56** | **0.86** 
(unadjusted) | (12.0, 19.3) | (-0.03, +0.25) | (0.87, 2.82) | (0.57, 1.29)

# Task 2. Fit the propensity score model, then plot the PS-treatment relationship

I'll use a logistic regression model

```{r}
psmodel <- glm(treated ~ covA + covB + covC + covD + covE + covF + 
                   Asqr + BC + BD, family=binomial(), data=toy)
arm::display(psmodel)
```

Having fit the model, my first step will be to save the raw and linear propensity score values to the main toy example tibble.

```{r}
toy$ps <- psmodel$fitted
toy$linps <- psmodel$linear.predictors
```

## Comparing the Distribution of Propensity Score Across the Two Treatment Groups

Now, I can use these saved values to assess the propensity model.

```{r}
by(toy$ps, toy$treated_f, summary)
```

The simplest plot is probably a boxplot, but it's not very granular.

```{r}
boxplot(toy$ps ~ toy$treated_f)
```

I'd rather get a fancier plot to compare the distributions of the propensity score across the two treatment groups, perhaps using a smoothed density estimate, as shown below. Here, I'll show the distributions of the linear propensity score, the log odds of treatment.

```{r}
ggplot(toy, aes(x = linps, fill = treated_f)) +
    geom_density(alpha = 0.3)
```

We see a fair amount of overlap across the two treatment groups. I'll use Rubin's Rules in the next section to help assess the amount of overlap at this point, before any adjustments for the propensity score.

# Task 3. Rubin's Rules to Check Overlap Before Propensity Adjustment

In his 2001 article[^1] about using propensity scores to design studies, as applied to studies of the causal effects of the conduct of the tobacco industry on medical expenditures, Donald Rubin proposed three "rules" for assessing the overlap / balance of covariates appropriately before and after propensity adjustment.  Before an outcome is evaluated using a regression analysis (perhaps supplemented by a propensity score adjustment through matching, weighting, subclassification or even direct adjustment), there are three checks that should be performed.

When we do a propensity score analysis, it will be helpful to perform these checks as soon as the propensity model has been estimated, even before any adjustments take place, to see how well the distributions of covariates overlap. After using the propensity score, we hope to see these checks meet the standards below. In what follows, I will describe each standard, and demonstrate its evaluation using the propensity score model we just fit, and looking at the original `toy` data set, without applying the propensity score in any way to do adjustments.

## Rubin's Rule 1

Rubin's Rule 1 states that the absolute value of the standardized difference of the linear propensity score, comparing the treated group to the control group, should be close to 0, ideally below 10%, and in any case less than 50%. If so, we may move on to Rule 2.

To evaluate this rule in the toy example, we'll run the following code to place the right value into a variable called `rubin1.unadj` (for Rubin's Rule 1, unadjusted).

```{r}
rubin1.unadj <- with(toy,
     abs(100*(mean(linps[treated==1])-mean(linps[treated==0]))/sd(linps)))
rubin1.unadj
```

What this does is calculate the (absolute value of the) standardized difference of the linear propensity score comparing treated subjects to control subjects. 

- We want this value to be close to 0, and certainly less than 50 in order to push forward to outcomes analysis without further adjustment for the propensity score. 
- Clearly, here, with a value of 88\%, we can't justify simply running an unadjusted regression model, be it a linear, logistic or Cox model - we've got observed selection bias, and need to actually apply the propensity score somehow in order to account for this. 
- So, we'll need to match, subclassify, weight or directly adjust for propensity here.

Since we've failed Rubin's 1st Rule, in some sense, we're done checking the rules, because we clearly need to further adjust for observed selection bias - there's no need to prove that further through checking Rubin's 2nd and 3rd rules. But we'll do it here to show what's involved.

[^1]: Rubin DB 2001 Using Propensity Scores to Help Design Observational Studies: Application to the Tobacco Litigation. Health Services & Outcomes Research Methodology 2: 169-188. Available on [our web site](https://sites.google.com/a/case.edu/love-500/home/reading-matter).

## Rubin's Rule 2

Rubin's Rule 2 states that the ratio of the variance of the linear propensity score in the treated group to the variance of the linear propensity score in the control group should be close to 1, ideally between 4/5 and 5/4, but certainly not very close to or exceeding 1/2 and 2. If so, we may move on to Rule 3.

To evaluate this rule in the toy example, we'll run the following code to place the right value into a variable called `rubin2.unadj` (for Rubin's Rule 2, unadjusted).

```{r}
rubin2.unadj <-with(toy, var(linps[treated==1])/var(linps[treated==0]))
rubin2.unadj
```

This is the ratio of variances of the linear propensity score comparing treated subjects to control subjects. We want this value to be close to 1, and certainly between 0.5 and 2. In this case, we pass Rule 2, if just barely.

## Rubin's Rule 3

For Rubin's Rule 3, we begin by calculating regression residuals for each covariate of interest (usually, each of those included in the propensity model) regressed on a single predictor - the linear propensity score. We then look to see if the ratio of the variance of the residuals of this model for the treatment group divided by the variance of the residuals of this model for the control group is close to 1. Again, ideally this will fall between 4/5 and 5/4 for each covariate, but certainly between 1/2 and 2. If so, then the use of regression models seems well justified.

To evaluate Rubin's 3rd Rule, we'll create a little function to help us do the calculations.

```{r rubin3 function}
## General function rubin3 to help calculate Rubin's Rule 3
rubin3 <- function(data, covlist, linps) {
  covlist2 <- as.matrix(covlist)
  res <- NA
  for(i in 1:ncol(covlist2)) {
    cov <- as.numeric(covlist2[,i])
    num <- var(resid(lm(cov ~ data$linps))[data$exposure == 1])
    den <- var(resid(lm(cov ~ data$linps))[data$exposure == 0])
    res[i] <- round(num/den, 3)
  }
  final <- data_frame(name = names(covlist), resid.var.ratio = res)
  return(final)
}
```

Now, then, applying the rule to our sample prior to propensity score adjustment, we get the following result. Note that I'm using the indicator variable forms for the `covF` information.

```{r}
cov.sub <- dplyr::select(toy,
                         covA, covB, covC, covD, covE,
                         covF.Middle, covF.High, Asqr, BC, BD)

toy$exposure <- toy$treated

rubin3.unadj <- rubin3(data = toy, covlist = cov.sub, linps = linps)
rubin3.unadj
```

Some of these covariates look to have residual variance ratios near 1, while others are further away, but all are within the (0.5, 2.0) range. So we'd pass Rule 3 here, although we'd clearly like to see some covariates (A and E, in particular) with ratios closer to 1.

### A Cleveland Dot Chart of the Rubin's Rule 3 Results

```{r rubin3 dot chart}
ggplot(rubin3.unadj, aes(x = resid.var.ratio, y = reorder(name, resid.var.ratio))) +
    geom_point(col = "blue") + 
    theme_bw() +
    xlim(0.5, 2.0) +
    geom_vline(aes(xintercept = 1)) +
    geom_vline(aes(xintercept = 4/5), linetype = "dashed", col = "red") +
    geom_vline(aes(xintercept = 5/4), linetype = "dashed", col = "red") +
  labs(x = "Residual Variance Ratio", y = "") 
```

We see several values between 0.5 and 0.8, but nothing outside (0.5, 2).

# Task 4. Use 1:1 greedy matching on the linear PS, then check post-match balance

As requested, we'll do 1:1 greedy matching on the linear propensity score without replacement and breaking ties randomly. To start, we won't include an outcome variable in our call to the `Match` function within the `Matching` package We'll wind up with a match including 70 treated and 70 control subjects.

```{r}
X <- toy$linps ## matching on the linear propensity score
Tr <- as.logical(toy$treated)
match1 <- Match(Tr=Tr, X=X, M = 1, replace=FALSE, ties=FALSE)
summary(match1)
```

## Balance Assessment (Semi-Automated)

Next, we'll assess the balance imposed by this greedy match on our covariates, and their transformations (`A`^2 and `B*C` and `B*D`) as well as the raw and linear propensity scores. The default output from the `MatchBalance` function is extensive...

```{r}
set.seed(5001)
mb1 <- MatchBalance(treated ~ covA + covB + covC + covD + covE + covF + 
                        Asqr + BC + BD + ps + linps, data=toy, 
                    match.out = match1, nboots=500)
```

The `cobalt` package has some promising tools for taking this sort of output and turning it into something useful. We'll look at that approach soon. For now, some old-school stuff...

## Extracting, Tabulating and Plotting Standardized Differences (without `cobalt`)

We'll start by naming the covariates that the `MatchBalance` output contains...

```{r}
covnames <- c("covA", "covB", "covC", "covD", "covE", 
              "covF - Middle", "covF - High", 
              "A^2","B*C", "B*D", "raw PS", "linear PS")
```

The next step is to extract the standardized differences (using the pooled denominator to estimate, rather than the treatment-only denominator used in the main output above.)

```{r}
pre.szd <- NULL; post.szd <- NULL
for(i in 1:length(covnames)) {
  pre.szd[i] <- mb1$BeforeMatching[[i]]$sdiff.pooled
  post.szd[i] <- mb1$AfterMatching[[i]]$sdiff.pooled
}
```

Now, we can build a table of the standardized differences:

```{r}
temp <- data.frame(pre.szd, post.szd, row.names=covnames)
print(temp, digits=3)
```

And then, more usefully, we can plot the absolute values of the standardized differences...

```{r}
temp <- data.frame(pre.szd, post.szd, row.names=covnames)
tempsort <- temp[with(temp, order(abs(pre.szd))),]
high <- max(max(abs(pre.szd)), max(abs(post.szd)), 0.1)

dotchart(abs(tempsort$pre.szd), pch="", xlim=c(0, 1.05*high), 
         labels=row.names(tempsort), main="Absolute Standardized Difference Plot",
         xlab="Absolute Standardized Difference (%)")
points(abs(tempsort$pre.szd), seq(1:length(tempsort$pre.szd)), 
       pch=15, col="blue", cex=1.2)
points(abs(tempsort$post.szd), seq(1:length(tempsort$post.szd)), 
       pch=19, col="red", cex=1.2)
abline(v=0, lty=1)
abline(v=10, lty=2, col="purple")
legend("bottomright", legend = c("Before Matching", "After Matching"), 
       col=c("blue", "red"), text.col=c("blue", "red"), bty="o", pch = c(15, 19))
```

Or, we can plot the standardized differences with their signs intact...

```{r}
temp <- data.frame(pre.szd, post.szd, row.names=covnames)
tempsort <- temp[with(temp, order(pre.szd)), ]
low <- min(min(pre.szd), min(post.szd), -0.1)
high <- max(max(pre.szd), max(post.szd), 0.1)

dotchart(tempsort$pre.szd, xlim=c(1.05*low, 1.05*high), pch="",
         labels=row.names(tempsort), main="Standardized Difference Plot",
         xlab="Standardized Difference (%)")
points(tempsort$pre.szd, seq(1:length(tempsort$pre.szd)), 
       pch=15, col="blue", cex=1.2)
points(tempsort$post.szd, seq(1:length(tempsort$post.szd)), 
       pch=19, col="red", cex=1.2)
abline(v=0, lty=1)
abline(v=10, lty=2, col="purple")
abline(v=-10, lty=2, col="purple")
legend("bottomright", legend = c("Before Matching", "After Matching"), 
       col=c("blue", "red"), text.col=c("blue", "red"), bty="o", pch = c(15, 19))
```

## Using `cobalt` to build a "Love Plot" after Matching

```{r }
b <- bal.tab(match1, treated ~ covA + covB + covC + covD + covE + covF + 
                        Asqr + BC + BD + ps + linps, data=toy, un = TRUE)
b
```

### Building a Plot of Standardized Differences, with `cobalt`

```{r }
p <- love.plot(b, threshold = .1, size = 1.5,
               var.order = "unadjusted",
               title = "Standardized Differences and 1:1 Matching")
p + theme_bw()
```

### Building a Plot of Variance Ratios, with `cobalt`

```{r }
p <- love.plot(b, stat = "v",
               threshold = 1.25, size = 1.5,
               var.order = "unadjusted",
               title = "Variance Ratios and 1:1 Matching")
p + theme_bw()
```

## Extracting, Tabulating and Plotting Variance Ratios (without `cobalt`)

Next, we extract the variance ratios, build a table and then plot them.

```{r}
pre.vratio <- NULL; post.vratio <- NULL
for(i in 1:length(covnames)) {
  pre.vratio[i] <- mb1$BeforeMatching[[i]]$var.ratio
  post.vratio[i] <- mb1$AfterMatching[[i]]$var.ratio
}

## Table of Variance Ratios
temp <- data.frame(pre.vratio, post.vratio, row.names=covnames)
print(temp, digits=2)

## Variance Ratio Plot
temp <- data.frame(pre.vratio, post.vratio, row.names=covnames)
tempsort <- temp[with(temp, order(pre.vratio)), ]
low <- min(min(pre.vratio), min(post.vratio))
high <- max(max(pre.vratio), max(post.vratio))

dotchart(tempsort$pre.vratio, xlim=c(0.95*low, 1.05*high), 
         pch="", labels=row.names(tempsort), main="Plot of Variance Ratios",
         xlab="Treatment Variance / Control Variance")
points(tempsort$pre.vratio, seq(1:length(tempsort$pre.vratio)), 
       pch=8, col="black", cex=1.2)
points(tempsort$post.vratio, seq(1:length(tempsort$post.vratio)), 
       pch=7, col="magenta", cex=1.2)
abline(v=1, lty=1)
abline(v=4/5, lty=2, col="brown")
abline(v=5/4, lty=2, col="brown")
legend("topleft", legend = c("Before Matching", "After Matching"), 
       col=c("black", "magenta"), text.col=c("black", "magenta"), 
       bty="o", pch = c(8, 7))
```

## Creating a New Data Frame, Containing the Matched Sample (without `cobalt`)

Now, we build a new matched sample data frame in order to do some of the analyses to come. This will contain only the 140 matched subjects (70 treated and 70 control).

```{r}
matches <- factor(rep(match1$index.treated, 2))
toy.matchedsample <- cbind(matches, toy[c(match1$index.control, match1$index.treated),])
```

Some sanity checks:

```{r}
table(toy.matchedsample$treated_f)
head(toy.matchedsample)
```

## Rubin's Rules to Check Balance After Matching

### Rubin's Rule 1

Rubin's Rule 1 states that the absolute value of the standardized difference of the linear propensity score, comparing the treated group to the control group, should be close to 0, ideally below 10%, and in any case less than 50%. If so, we may move on to Rule 2.

Recall that our result without propensity matching (or any other adjustment) was 

```{r}
rubin1.unadj
```

To run this for our matched sample, we use:

```{r}
rubin1.match <- with(toy.matchedsample,
      abs(100*(mean(linps[treated==1])-mean(linps[treated==0]))/sd(linps)))
rubin1.match
```

Here, we've at least got this value down below 50\%, so we would pass Rule 1, although perhaps a different propensity score adjustment (perhaps by weighting or subclassification, or using a different matching approach) might improve this result by getting it closer to 0.

### Rubin's Rule 2

Rubin's Rule 2 states that the ratio of the variance of the linear propensity score in the treated group to the variance of the linear propensity score in the control group should be close to 1, ideally between 4/5 and 5/4, but certainly not very close to or exceeding 1/2 and 2. If so, we may move on to Rule 3.

Recall that our result without propensity matching (or any other adjustment) was 

```{r}
rubin2.unadj
```

To run this for our matched sample, we use:

```{r}
rubin2.match <- with(toy.matchedsample, var(linps[treated==1])/var(linps[treated==0]))
rubin2.match
```

This is moderately promising - a substantial improvement over our unadjusted result, though not yet within our desired range of 4/5 to 5/4, but clearly within 1/2 to 2. 
We pass Rule 2, as well.

### Rubin's Rule 3

For Rubin's Rule 3, we begin by calculating regression residuals for each covariate of interest (usually, each of those included in the propensity model) regressed on a single predictor - the linear propensity score. We then look to see if the ratio of the variance of the residuals of this model for the treatment group divided by the variance of the residuals of this model for the control group is close to 1. Again, ideally this will fall between 4/5 and 5/4 for each covariate, but certainly between 1/2 and 2. If so, then the use of regression models seems well justified.

Recall that our result without propensity matching (or any other adjustment) was 

```{r}
rubin3.unadj
```

After propensity matching, we use this code to assess Rubin's 3rd Rule in our matched sample.

```{r}
cov.sub <- dplyr::select(toy.matchedsample,
                         covA, covB, covC, covD, covE,
                         covF.Middle, covF.High, Asqr, BC, BD)

toy.matchedsample$exposure <- toy.matchedsample$treated

rubin3.matched <- rubin3(data = toy.matchedsample, covlist = cov.sub, linps = linps)

rubin3.matched
```

It looks like the results are basically unchanged, except that `covF.High` is improved. The dotplot of these results comparing pre- to post-matching is shown below.

### A Cleveland Dot Chart of the Rubin's Rule 3 Results Pre vs. Post-Match

```{r rubin3 dot chart pre and post match}
rubin3.both <- bind_rows(rubin3.unadj, rubin3.matched)
rubin3.both$source <- c(rep("Unmatched",10), rep("Matched", 10))

ggplot(rubin3.both, aes(x = resid.var.ratio, y = name, col = source)) +
    geom_point() + 
    theme_bw() +
    xlim(0.5, 2.0) +
    geom_vline(aes(xintercept = 1)) +
    geom_vline(aes(xintercept = 4/5), linetype = "dashed", col = "red") +
    geom_vline(aes(xintercept = 5/4), linetype = "dashed", col = "red") +
  labs(x = "Residual Variance Ratio", y = "") 
```

Not a whole lot of improvement to report.

# Task 5. After matching, estimate the causal effect of treatment on ...

## Outcome 1 (a continuous outcome)

### Approach 1. Automated Approach from the Matching Library - ATT Estimate

First, we'll look at the essentially automatic answer which can be obtained when using the `Matching` package and inserting an outcome Y. For a continuous outcome, this is often a reasonable approach.

```{r}
X <- toy$linps ## matching on the linear propensity score
Tr <- as.logical(toy$treated)
Y <- toy$out1.cost
match1 <- Match(Y=Y, Tr=Tr, X=X, M = 1, replace=FALSE, ties=FALSE)
summary(match1)
```

We can obtain an approximate 95% confidence interval by adding and subtracting 1.96 times (or just double) the standard error (SE) to the point estimate, which is 15.6 here. Here, using the 1.96 figure, that would yields an approximate 95% CI of (11.6, 19.6).

### Approach 2. Automated Approach from the Matching Library - ATE Estimate

```{r}
match1.ATE <- Match(Y=Y, Tr=Tr, X=X, M = 1, replace=FALSE, ties=FALSE, estimand="ATE")
summary(match1.ATE)
rm(match1.ATE)
```

And our 95% CI for this ATE estimate would be 16.014 $\pm$ 1.96(1.5614), or (13.0, 19.1), but we'll stick with the ATT estimate for now.

### ATT vs. ATE: Definitions

- Informally, the **average treatment effect on the treated** (ATT) estimate describes the difference in potential outcomes (between treated and untreated subjects) summarized across the population of people who actually received the treatment. 
    + In our initial match, we identified a unique and nicely matched control patient for each of the 70 people in the treated group. We have a 1:1 match on the treated, and thus can describe subjects across that set of treated patients reasonably well.
- On the other hand the **average treatment effect** (ATE) refers to the difference in potential outcomes summarized across the entire population, including those who did not receive the treatment.  
    + In our ATE match, we have less success, in part because if we match to the treated patients in a 1:1 way, we'll have an additional 60 unmatched control patients, about whom we can describe results only vaguely. We could consider matching up control patients to treated patients, perhaps combined with a willingness to re-use some of the treated patients to get a better estimate across the whole population.

### Approach 3. Mirroring the Paired T test in a Regression Model

We can mirror the paired t test result in a regression model that treats the match identifier as a fixed factor in a linear model, as follows. This takes the pairing into account, but treating pairing as a fixed, rather than random, factor, isn't really satisfactory as a solution, although it does match the paired t test.

```{r}
adj.m.out1 <- lm(out1.cost ~ treated + factor(matches), data=toy.matchedsample) 
coef(adj.m.out1)["treated"] # point estimate for treated effect
confint(adj.m.out1)["treated",1] # lower limit of 95% CI
confint(adj.m.out1)["treated",2] # lower limit of 95% CI
```

So, this regression approach produces an estimate that is exactly the same as the paired t test[^2], but this isn't something I'm completely comfortable with.

[^2]: I'll leave checking that this is true as an exercise for the curious.

### Approach 4. A Mixed Model to account for 1:1 Matching

What I think of as a more appropriate result comes from a mixed model where the matches are treated as a random factor, but the treatment group is treated as a fixed factor. This is developed like this, using the `lme4` package. Note that we have to create a factor variable to represent the matches, since that's the only thing that `lme4` understands.

```{r}
toy.matchedsample$matches.f <- as.factor(toy.matchedsample$matches) 
## Need to use matches as a factor in R here

mixedmodel.out1 <- lmer(out1.cost ~ treated + (1 | matches.f), data=toy.matchedsample)
summary(mixedmodel.out1); confint(mixedmodel.out1)
```

### Practically, does any of this matter in this example?

Not in this example, no, as long as you stick to the ATT approaches.

Approach | Effect Estimate | Standard Error | 95\% CI
----------------------: | ---------: | --------: | ---------------
"Automated" ATT via `Match` | 15.56 | 2.04 | 11.56, 19.55
Linear Model (pairs as fixed factor) | 15.56 | 2.05 | 11.46, 19.66
Mixed Model (pairs as random factor) | 15.57 | 2.05 | 11.50, 19.61


## Outcome 2 (a binary outcome)

### Approach 1. Automated Approach from the Matching Library (ATT)

First, we'll look at the essentially automatic answer which can be obtained when using the `Matching` package and inserting an outcome Y. For a binary outcome, this is often a reasonable approach, especially if you don't wish to adjust for any other covariate, and the result will be expressed as a risk difference, rather than as a relative risk or odds ratio. Note that I have used the 0-1 version of Outcome 2, rather than a factor version. The estimate produced is the difference in risk associated with `out2` = 1 (Treated subjects) minus `out2` = 0 (Controls.)

```{r}
X <- toy$linps ## matching on the linear propensity score
Tr <- as.logical(toy$treated)
Y <- toy$out2
match1 <- Match(Y=Y, Tr=Tr, X=X, M = 1, replace=FALSE, ties=FALSE)
summary(match1)
```

As in the continuous case, we obtain an approximate 95\% confidence interval by adding and subtracting 1.96 times (or just double) the standard error (SE) to the point estimate, which is 0.1 (i.e. 10 percentage points) here. 

- Here, using the 1.96 figure, that would yields an approximate 95% CI of (-0.05, 0.25).
- Again, the estimated average causal effect is not statistically significant here, since 0 is contained in this confidence interval.

### Approach 2. Using the matched sample to perform a conditional logistic regression

Since we have the matched sample available, we can simply perform a conditional logistic regression to estimate the treatment effect in terms of a log odds ratio (or, by exponentiating, an odds ratio.) Again, I use the 0/1 version of both the outcome and treatment indicator. The key modeling function `clogit` is part of the `survival` package.

```{r}
adj.m.out2 <- clogit(out2 ~ treated + strata(matches), data=toy.matchedsample)
summary(adj.m.out2)
```

The odds ratio in the `exp(coef)` section above is the average causal effect estimate - it describes the odds of having an event (`out2`) occur associated with being a treated subject, as compared to the odds of the event when a control subject. 

- Again, the result, though nominally fairly far from 1, is nowhere near statistically significant, according to our 95% confidence interval. 
- Our estimate is 1.64, with 95% CI (0.77, 3.47). 
- I'll use this conditional logistic regression approach to summarize the findings with regard to an odds ratio in my summary of matching results to come.

## Outcome 3 (a time-to-event outcome)

### Approach 1. Automated Approach from the Matching Library

Again, we'll start by thinking about the essentially automatic answer which can be obtained when using the `Match` function. The problem here is that this approach doesn't take into account the right censoring at all, and assumes that all of the specified times in Outcome 3 are observed. This causes the result (or the ATE version) to be non-sensical, given what we know about the data. So I don't recommend you use this approach when dealing with a time-to-event outcome.

And as a result, I won't even shos it here.

### Approach 2. A stratified Cox proportional hazards model

Since we have the matched sample, we can use a stratified Cox proportional hazards model to compare the treatment groups on our time-to-event outcome, while accounting for the matched pairs. The main results will be a relative hazard rate estimate, with 95% CI. Again, I use the 0/1 numeric version of the event indicator (`out2`), and of the treatment indicator (`treated`) here.

```{r}
adj.m.out3 <- coxph(Surv(out3.time, out2) ~ treated + strata(matches), data=toy.matchedsample)
summary(adj.m.out3)
```

The relative hazard rate (from the `exp(coef)` section of the output) is estimated to be 0.75, with 95% CI (0.41, 1.39) - again not statistically significant. Checking the proportional hazards assumption suggests a possible issue, as well...

```{r}
cox.zph(adj.m.out3) # Quick check for proportional hazards assumption
plot(cox.zph(adj.m.out3), var="treated")
```

## Results So Far (After Propensity Matching)

So, here's our summary again, now incorporating both our unadjusted results and the results after matching. Automated results and my favorite of our various non-automated approaches are shown. Note that I've left out the "automated" approach for a time-to-event outcome entirely, so as to discourage you from using it as presented above. 

Est. Treatment Effect (95\% CI) | Outcome 1 (Cost diff.) | Outcome 2 (Risk diff.) | Outcome 2 (Odds Ratio) | Outcome 3 (Relative Hazard Rate)
----------------: | -----------: | -----------: | -----------: | -----------: 
No covariate adjustment | **15.7** | **+0.11** | **1.56** | **0.86** 
(unadjusted) | (12.0, 19.3) | (-0.03, +0.25) | (0.87, 2.82) | (0.57, 1.29)
After 1:1 PS Match | **15.6** | **+0.11** | N/A | N/A
(`Match`: Automated) | (11.6, 19.6) | (-0.05, +0.25) | N/A | N/A
After 1:1 PS Match | **15.6** | N/A | **1.64** | **0.75**
("Regression" Models) | (11.5, 19.6) | N/A | (0.77, 3.47) | (0.41, 1.38)

# Task 6. Subclassify by PS quintile, then display post-subclassification balance

First, we divide the data by the propensity score into 5 strata of equal size using the `cut2` function from the `Hmisc` package. Then we create a `quintile` variable which specifies 1 = lowest propensity scores to 5 = highest.

```{r}
toy$stratum <- cut2(toy$ps, g=5)

by(toy$ps, toy$stratum, summary) # sanity check

toy$quintile <- factor(toy$stratum, labels=1:5)

table(toy$stratum, toy$quintile) ## sanity check
```

## Check Balance and Propensity Score Overlap in Each Quintile

We want to check the balance and propensity score overlap for each quintile. I'll start with a set of facetted, jittered plots to look at overlap.

```{r}
ggplot(toy, aes(x = treated_f, y = round(ps,2), group = quintile, color = treated_f)) +
    geom_jitter(width = 0.2) +
    guides(color = FALSE) +
    facet_wrap(~ quintile) +
    labs(x = "", y = "Propensity for Treatment", 
         title = "Quintile Subclassification in the Toy Example")
```

It can be helpful to know how many observations (by exposure group) are in each quintile.

```{r}
addmargins(table(toy$quintile, toy$treated_f))
```

The overlap may show a little better in the plot if you free up the y axes...

```{r}
ggplot(toy, aes(x = treated_f, y = round(ps,2), group = quintile, color = treated_f)) +
    geom_jitter(width = 0.2) +
    guides(color = FALSE) +
    facet_wrap(~ quintile, scales = "free_y") +
    labs(x = "", y = "Propensity for Treatment", 
         title = "Quintile Subclassification in the Toy Example")
```

## Creating a Standardized Difference Calculation Function

We'll need to be able to calculate standardized differences in this situation so I've created a simple `szd` function to do this - using the average denominator method.

```{r}
szd <- function(covlist, g) {
  covlist2 <- as.matrix(covlist)
  g <- as.factor(g)
  res <- NA
  for(i in 1:ncol(covlist2)) {
    cov <- as.numeric(covlist2[,i])
    num <- 100*diff(tapply(cov, g, mean, na.rm=TRUE))
    den <- sqrt(mean(tapply(cov, g, var, na.rm=TRUE)))
    res[i] <- round(num/den,2)
  }
  names(res) <- names(covlist)   
  res
}
```


## Creating the Five Subsamples, by PS Quintile

Next, we split the complete sample into the five quintiles.

```{r}
## Divide the sample into the five quintiles
quin1 <- filter(toy, quintile==1)
quin2 <- filter(toy, quintile==2)
quin3 <- filter(toy, quintile==3)
quin4 <- filter(toy, quintile==4)
quin5 <- filter(toy, quintile==5)
```

## Standardized Differences in Each Quintile, and Overall

Now, we'll calculate the standardized differences within each quintile, as well as overall.

```{r}
covs <- c("covA", "covB", "covC", "covD", "covE", "covF.Middle", "covF.High", "Asqr","BC", "BD", "ps", "linps")
d.q1 <- szd(quin1[covs], quin1$treated)
d.q2 <- szd(quin2[covs], quin2$treated)
d.q3 <- szd(quin3[covs], quin3$treated)
d.q4 <- szd(quin4[covs], quin4$treated)
d.q5 <- szd(quin5[covs], quin5$treated)
d.all <- szd(toy[covs], toy$treated)

toy.szd <- data_frame(covs, Overall = d.all, Q1 = d.q1, Q2 = d.q2, Q3 = d.q3, Q4 = d.q4, Q5 = d.q5)
toy.szd <- gather(toy.szd, "quint", "sz.diff", 2:7)
toy.szd
```

## Plotting the Standardized Differences

```{r}
ggplot(toy.szd, aes(x = sz.diff, y = reorder(covs, -sz.diff), group = quint)) + 
    geom_point() +
    geom_vline(xintercept = 0) +
    geom_vline(xintercept = c(-10,10), linetype = "dashed", col = "blue") +
    facet_wrap(~ quint) +
    labs(x = "Standardized Difference, %", y = "",
         title = "Comparing Standardized Differences by PS Quintile",
         subtitle = "The toy example")
```


```{r}
ggplot(toy.szd, aes(x = abs(sz.diff), y = covs, group = quint)) + 
    geom_point() +
    geom_vline(xintercept = 0) +
    geom_vline(xintercept = c(-10,10), linetype = "dashed", col = "blue") +
    facet_wrap(~ quint) +
    labs(x = "|Standardized Difference|, %", y = "",
         title = "Absolute Standardized Differences by PS Quintile",
         subtitle = "The toy example")
```

## Checking Rubin's Rules Post-Subclassification

### Rubin's Rule 1

As a reminder, prior to adjustment, Rubin's Rule 1 for the `toy` example was:

```{r}
rubin1.unadj <- with(toy,
                     abs(100*(mean(linps[treated==1]) -
                                  mean(linps[treated==0]))/sd(linps)))
rubin1.unadj
```

After propensity score subclassification, we can obtain the same summary within each of the five quintiles...

```{r}
rubin1.q1 <- with(quin1, abs(100*(mean(linps[treated==1]) - 
                                      mean(linps[treated==0]))/sd(linps)))
rubin1.q2 <- with(quin2, abs(100*(mean(linps[treated==1]) - 
                                      mean(linps[treated==0]))/sd(linps)))
rubin1.q3 <- with(quin3, abs(100*(mean(linps[treated==1]) - 
                                      mean(linps[treated==0]))/sd(linps)))
rubin1.q4 <- with(quin4, abs(100*(mean(linps[treated==1]) - 
                                      mean(linps[treated==0]))/sd(linps)))
rubin1.q5 <- with(quin5, abs(100*(mean(linps[treated==1]) - 
                                      mean(linps[treated==0]))/sd(linps)))

rubin1.sub <- c(rubin1.q1, rubin1.q2, rubin1.q3, rubin1.q4, rubin1.q5)
names(rubin1.sub)=c("Q1", "Q2", "Q3", "Q4", "Q5")

rubin1.sub
```

Each quintile shows (in some cases, only a slightly) better result than the full data set. With a small sample size like this, and some subclasses having very few subjects in one exposure group, it was always a long shot that subclassification alone would reduce all of these values below 10\%, but I had hoped to get more than 3/5 below 50\%. 

### Rubin's Rule 2

As a reminder, prior to adjustment, Rubin's Rule 2 for the `toy` example was:

```{r}
rubin2.unadj <- with(toy, var(linps[treated==1])/var(linps[treated==0]))
rubin2.unadj
```

After Subclassification, we can obtain the same summary within each of the five quintiles...

```{r}
rubin2.q1 <- with(quin1, var(linps[treated==1])/var(linps[treated==0]))
rubin2.q2 <- with(quin2, var(linps[treated==1])/var(linps[treated==0]))
rubin2.q3 <- with(quin3, var(linps[treated==1])/var(linps[treated==0]))
rubin2.q4 <- with(quin4, var(linps[treated==1])/var(linps[treated==0]))
rubin2.q5 <- with(quin5, var(linps[treated==1])/var(linps[treated==0]))

rubin2.sub <- c(rubin2.q1, rubin2.q2, rubin2.q3, rubin2.q4, rubin2.q5)
names(rubin2.sub)=c("Q1", "Q2", "Q3", "Q4", "Q5")

rubin2.sub
```

Some of these variance ratios are actually a bit further from 1 than the full data set. Again, with a small sample size like this, subclassification looks like a weak choice. At most, three of the quintiles (2-4) show OK variance ratios after propensity score subclassification.

### Rubin's Rule 3

Prior to propensity adjustment, recall that Rubin's Rule 3 summaries were:

```{r}
covs <- c("covA", "covB", "covC", "covD", "covE", 
          "covF.Middle", "covF.High", "Asqr","BC", "BD")
rubin3.unadj <- rubin3(data=toy, covlist=toy[covs])
```

After subclassification, then, Rubin's Rule 3 summaries within each quintile are:

```{r}
rubin3.q1 <- rubin3(data=quin1, covlist=quin1[covs])
rubin3.q2 <- rubin3(data=quin2, covlist=quin2[covs])
rubin3.q3 <- rubin3(data=quin3, covlist=quin3[covs])
rubin3.q4 <- rubin3(data=quin4, covlist=quin4[covs])
rubin3.q5 <- rubin3(data=quin5, covlist=quin5[covs])

toy.rubin3 <- data_frame(covs, All = rubin3.unadj$resid.var.ratio, 
                         Q1 = rubin3.q1$resid.var.ratio, 
                         Q2 = rubin3.q2$resid.var.ratio, 
                         Q3 = rubin3.q3$resid.var.ratio, 
                         Q4 = rubin3.q4$resid.var.ratio, 
                         Q5 = rubin3.q5$resid.var.ratio)

toy.rubin3 <- gather(toy.rubin3, "quint", "rubin3", 2:7)
```


```{r}
ggplot(toy.rubin3, aes(x = rubin3, y = covs, group = quint)) + 
    geom_point() +
    geom_vline(xintercept = 1) +
    geom_vline(xintercept = c(0.8, 1.25), linetype = "dashed", col = "blue") +
    geom_vline(xintercept = c(0.5, 2), col = "red") +
    facet_wrap(~ quint) +
    labs(x = "Residual Variance Ratio", y = "",
         title = "Residual Variance Ratios by PS Quintile",
         subtitle = "Rubin's Rule 3: The toy example")
```

Most of the residual variance ratios are in the range of (0.5, 2) but the results within quintiles vary widely. Quintiles 1 and 2 are especially problematic in this regard.

# Task 7. After subclassifying, what is the estimated average treatment effect?

## ... on Outcome 1 [a continuous outcome]

First, we'll find the estimated average causal effect (and standard error) within each quintile via linear regression.

```{r}
quin1.out1 <- lm(out1.cost ~ treated, data=quin1)
quin2.out1 <- lm(out1.cost ~ treated, data=quin2)
quin3.out1 <- lm(out1.cost ~ treated, data=quin3)
quin4.out1 <- lm(out1.cost ~ treated, data=quin4)
quin5.out1 <- lm(out1.cost ~ treated, data=quin5)

coef(summary(quin1.out1)); coef(summary(quin2.out1)); coef(summary(quin3.out1)); coef(summary(quin4.out1)); coef(summary(quin5.out1))
```

We could probably figure out a cleverer way to accomplish this using the `broom` package.

Next, we find the mean of the five quintile-specific estimated regression coefficients

```{r}
est.st <- (coef(quin1.out1)[2] + coef(quin2.out1)[2] + coef(quin3.out1)[2] +
               coef(quin4.out1)[2] + coef(quin5.out1)[2])/5
est.st
```

To get the combined standard error estimate, we do the following:

```{r}
se.q1 <- summary(quin1.out1)$coefficients[2,2]
se.q2 <- summary(quin2.out1)$coefficients[2,2]
se.q3 <- summary(quin3.out1)$coefficients[2,2]
se.q4 <- summary(quin4.out1)$coefficients[2,2]
se.q5 <- summary(quin5.out1)$coefficients[2,2]

se.st <- sqrt((se.q1^2 + se.q2^2 + se.q3^2 + se.q4^2 + se.q5^2)*(1/25))
se.st
```

The resulting 95% confidence Interval for the average causal treatment effect is then:

```{r}
temp.result1 <- c(est.st, est.st - 1.96*se.st, est.st + 1.96*se.st)
names(temp.result1) <- c("Estimate", "Low 95% CI", "High 95% CI")
temp.result1
```

## ... on Outcome 2 [a binary outcome]

First, we find the estimated average causal effect (and standard error) within each quintile via logistic regression:

```{r}
quin1.out2 <- glm(out2 ~ treated, data=quin1, family=binomial())
quin2.out2 <- glm(out2 ~ treated, data=quin2, family=binomial())
quin3.out2 <- glm(out2 ~ treated, data=quin3, family=binomial())
quin4.out2 <- glm(out2 ~ treated, data=quin4, family=binomial())
quin5.out2 <- glm(out2 ~ treated, data=quin5, family=binomial())

coef(summary(quin1.out2)); coef(summary(quin2.out2)); coef(summary(quin3.out2)); coef(summary(quin4.out2)); coef(summary(quin5.out2))
```

Next, we find the mean of the five quintile-specific estimated logistic regression coefficients

```{r}
est.st <- (coef(quin1.out2)[2] + coef(quin2.out2)[2] + coef(quin3.out2)[2] +
               coef(quin4.out2)[2] + coef(quin5.out2)[2])/5
est.st ## this is the estimated log odds ratio

## And we exponentiate this to get the overall odds ratio estimate
exp(est.st)
```

To get the combined standard error estimate across the five quintiles, we do the following:

```{r}
se.q1 <- summary(quin1.out2)$coefficients[2,2]
se.q2 <- summary(quin2.out2)$coefficients[2,2]
se.q3 <- summary(quin3.out2)$coefficients[2,2]
se.q4 <- summary(quin4.out2)$coefficients[2,2]
se.q5 <- summary(quin5.out2)$coefficients[2,2]
se.st <- sqrt((se.q1^2 + se.q2^2 + se.q3^2 + se.q4^2 + se.q5^2)*(1/25))
se.st
## Of course, this standard error is also on the log odds ratio scale
```

Now, we obtain a 95\% Confidence Interval for the Average Causal Effect of our treatment (as an Odds Ratio) through combination and exponentiation, as follows:

```{r}
temp.result2 <- c(exp(est.st), exp(est.st - 1.96*se.st), exp(est.st + 1.96*se.st))
names(temp.result2) <- c("Estimate", "Low 95% CI", "High 95% CI")
temp.result2
```

## ... on Outcome 3 [a time to event]

Subjects with `out2.event` = "Yes" are truly observed events, while those with `out2.event` == "No" are censored before an event can happen to them.

The Cox model comparing treated to control, stratifying on quintile, is...

```{r}
adj.s.out3 <- coxph(Surv(out3.time, out2) ~ treated + strata(quintile), data=toy)
summary(adj.s.out3) ## exp(coef) gives relative hazard associated with treatment
```

### Checking the Proportional Hazards Assumption

The proportional hazards assumption looks fairly reasonable.

```{r}
cox.zph(adj.s.out3) ## checking the proportional hazards assumption
plot(cox.zph(adj.s.out3), var="treated")
```

## Results So Far (After Matching and Subclassification)

These subclassification results describe the average treatment effect, while the previous analyses we have completed describe the average treatment effect on the treated. This is one reason for the meaningful difference between the estimates.

Est. Treatment Effect (95\% CI) | Outcome 1 (Cost diff.) | Outcome 2 (Risk diff.) | Outcome 2 (Odds Ratio) | Outcome 3 (Relative Hazard Rate)
----------------: | -----------: | -----------: | -----------: | -----------: 
No covariate adjustment | **15.7** | **+0.11** | **1.56** | **0.86** 
(unadjusted) | (12.0, 19.3) | (-0.03, +0.25) | (0.87, 2.82) | (0.57, 1.29)
After 1:1 PS Match | **15.6** | **+0.11** | N/A | N/A
(`Match`: Automated) | (11.6, 19.6) | (-0.05, +0.25) | N/A | N/A
After 1:1 PS Match | **15.6** | N/A | **1.64** | **0.75**
("Regression" models) | (11.5, 19.6) | N/A | (0.77, 3.47) | (0.41, 1.38)
After PS Subclassification | **7.9** | N/A | **1.15** | **0.79**
("Regression" models, ATE) | (4.1, 11.7) | N/A | (0.54, 2.44) | (0.49, 1.27)

# Task 8. Execute weighting by the inverse PS, then assess covariate balance

## ATT approach: Weight treated subjects as 1; control subjects as ps/(1-ps)

```{r}
toy$wts1 <- ifelse(toy$treated==1, 1, toy$ps/(1-toy$ps))
```

Here is a plot of the resulting ATT (average treatment effect on the treated) weights:

```{r}
ggplot(toy, aes(x = ps, y = wts1, color = treated_f)) +
    geom_point() + 
    guides(color = FALSE) +
    facet_wrap(~ treated_f) +
    labs(x = "Estimated Propensity for Treatment",
         y = "ATT weights for the toy example",
         title = "ATT weighting structure: Toy example")
```


## ATE Approach: Weight treated subjects by 1/ps; Control subjects by 1/(1-PS)

```{r}
toy$wts2 <- ifelse(toy$treated==1, 1/toy$ps, 1/(1-toy$ps))
```

Here's a plot of the ATE (average treatment effect) weights...

```{r}
ggplot(toy, aes(x = ps, y = wts2, color = treated_f)) +
    geom_point() + 
    guides(color = FALSE) +
    facet_wrap(~ treated_f) +
    labs(x = "Estimated Propensity for Treatment",
         y = "ATE weights for the toy example",
         title = "ATE weighting structure: Toy example")
```

## Assessing Balance after Weighting

The `twang` package provides several functions for assessing balance after weighting, in addition to actually doing the weighting using more complex propensity models. For this example, we'll demonstrate balance assessment for our two (relatively simple) weighting schemes. In other examples, we'll use `twang` to do more complete weighting work.

### Reminder of ATT vs. ATE Definitions

- Informally, the **average treatment effect on the treated** (ATT) estimate describes the difference in potential outcomes (between treated and untreated subjects) summarized across the population of people who actually received the treatment. This is usually the estimate we work with in making causal estimates from observational studies.
- On the other hand, the **average treatment effect** (ATE) refers to the difference in potential outcomes summarized across the entire population, including those who did not receive the treatment.  


### For ATT weights (`wts1`)

```{r}
toy_df <- data.frame(toy) # twang doesn't react well to tibbles

covlist <- c("covA", "covB", "covC", "covD", "covE", "covF", "Asqr","BC", "BD", "ps", "linps")

# for ATT weights
bal.wts1 <- dx.wts(x=toy_df$wts1, data=toy_df, vars=covlist, 
                   treat.var="treated", estimand="ATT")
bal.wts1
bal.table(bal.wts1)
```

The `std.eff.sz` shows the standardized difference, but as a proportion, rather than as a percentage. We'll create a data frame (tibble) so we can plot the data more easily.

```{r }
bal.before.wts1 <- bal.table(bal.wts1)[1]
bal.after.wts1 <- bal.table(bal.wts1)[2]

balance.att.weights <- data_frame(names = rownames(bal.before.wts1$unw), 
                              pre.weighting = 100*bal.before.wts1$unw$std.eff.sz, 
                              ATT.weighted = 100*bal.after.wts1[[1]]$std.eff.sz)
balance.att.weights <- gather(balance.att.weights, timing, szd, 2:3)
```

OK - here is the plot of standardized differences before and after ATT weighting.

```{r}
ggplot(balance.att.weights, aes(x = szd, y = reorder(names, szd), color = timing)) +
    geom_point() + 
    geom_vline(xintercept = 0) +
    geom_vline(xintercept = c(-10,10), linetype = "dashed", col = "blue") +
    labs(x = "Standardized Difference", y = "", 
         title = "Standardized Difference before and after ATT Weighting",
         subtitle = "The toy example") 
```


### For ATE weights (`wts2`)

```{r}
bal.wts2 <- dx.wts(x=toy_df$wts2, data=toy_df, vars=covlist, 
                   treat.var="treated", estimand="ATE")
bal.wts2
bal.table(bal.wts2)
```

```{r }
bal.before.wts2 <- bal.table(bal.wts2)[1]
bal.after.wts2 <- bal.table(bal.wts2)[2]

balance.ate.weights <- data_frame(names = rownames(bal.before.wts2$unw), 
                              pre.weighting = 100*bal.before.wts2$unw$std.eff.sz, 
                              ATT.weighted = 100*bal.after.wts2[[1]]$std.eff.sz)
balance.ate.weights <- gather(balance.ate.weights, timing, szd, 2:3)
```

Here is the plot of standardized differences before and after ATE weighting.

```{r}
ggplot(balance.ate.weights, aes(x = szd, y = reorder(names, szd), color = timing)) +
    geom_point() + 
    geom_vline(xintercept = 0) +
    geom_vline(xintercept = c(-10,10), linetype = "dashed", col = "blue") +
    labs(x = "Standardized Difference", y = "", 
         title = "Standardized Difference before and after ATE Weighting",
         subtitle = "The toy example") 
```

## Rubin's Rules after ATT weighting

For our weighted sample, our summary statistic for Rules 1 and 2 may be found from the
`bal.table` output.

### Rubin's Rule 1

We can read off the standardized effect size after weighting for the linear propensity
score as 0.062. Multiplying by 100, we get 6.2\%, so we would pass Rule 1.

### Rubin's Rule 2

We can read off the standard deviations within the treated and control groups. We can
then square each, to get the relevant variances, then take the ratio of those variances.
Here, we have 0.917^2 / 0.838^2 = 1.1974314, which is a substantial improvement
over our unadjusted result, and now within our desired range of 4/5 to 5/4, as well as
clearly within 1/2 to 2. Pass Rule 2, also.

### Rubin's Rule 3

Rubin's Rule 3 requires some more substantial manipulation of the data. I'll skip that for now.

## Rubin's Rules after ATE weighting

Again, our summary statistic for Rules 1 and 2 may be found from the `bal.table` output.

### Rubin's Rule 1

The standardized effect size after ATE weighting for the linear propensity score is 0.050. Multiplying by 100, we get 5.0%, so we would pass Rule 1.

### Rubin's Rule 2

We can read off the standard deviations within the treated and control groups from the ATE weights, then square to get the variances, then take the ratio. Here, we have 1.133^2 / 1.193^2 = 0.9019427, which is also a substantial improvement over our unadjusted result, and within our desired range of 4/5 to 5/4. Pass Rule 2, also.

### Rubin's Rule 3

Again, for now, I'm skipping Rubin's Rule 3 after weighting.

# Using TWANG for Alternative PS Estimation and ATT Weighting

Here, I'll demonstrate the use of the the `twang` package's functions to fit the propensity model and then perform ATT weighting, mostly using default options.

## Estimate the Propensity Score using Generalized Boosted Regression, and then perfom ATT Weighting

We can directly use the `twang` (**t**oolkit for **w**eighting and **a**nalysis of **n**onequivalent **g**roups) package to weight our results, and even to re-estimate the propensity score using generalized boosted regression rather than a logistic regression model. The `twang` vignette is very helpful and found at [this link](https://cran.r-project.org/web/packages/twang/vignettes/twang.pdf).

To begin, we'll estimate the propensity score using the `twang` function `ps`. This uses a *generalized boosted regression* approach to estimate the propensity score and produce material for checking balance.

```{r, cache=TRUE, warning = FALSE}
# Recall that twang does not play well with tibbles,
# so we have to use the data frame version of the toy object

ps.toy <- ps(treated ~ covA + covB + covC + covD + covE + covF + 
                 Asqr + BC + BD,
             data = toy_df,
             n.trees = 2000,
             interaction.depth = 2,
             stop.method = c("es.mean"),
             estimand = "ATT",
             verbose = FALSE)
```

### Did we let the simulations run long enough to stabilize estimates?

```{r}
plot(ps.toy)
```

### What is the effective sample size of our weighted results?

```{r}
summary(ps.toy)
```

### How is the balance?

```{r}
plot(ps.toy, plots = 2)
```

```{r}
plot(ps.toy, plots = 3)
```

### Assessing Balance with `cobalt`

```{r}
bal.tab(ps.toy, full.stop.method = "es.mean.att")
```

## Semi-Automated Love plot of Standardized Differences

```{r}
p <- love.plot(bal.tab(ps.toy), 
               threshold = .1, size = 1.5, 
               title = "Standardized Diffs and TWANG ATT weighting")
p + theme_bw()
```

## Semi-Automated Love plot of Variance Ratios

```{r}
p <- love.plot(bal.tab(ps.toy), stat = "v",
               threshold = 1.25, size = 1.5, 
               title = "Variance Ratios: TWANG ATT weighting")
p + theme_bw()
```

# Task 9. After weighting, what is the estimated average average causal effect of treatment?

## ... on Outcome 1 [a continuous outcome]

### with ATT weights

The relevant regression approach uses the `svydesign` and `svyglm` functions from the `survey` package.

```{r}
toywt1.design <- svydesign(ids=~1, weights=~wts1, data=toy) # using ATT weights

adjout1.wt1 <- svyglm(out1.cost ~ treated, design=toywt1.design)
summary(adjout1.wt1); confint(adjout1.wt1)
```

### with ATE weights

```{r}
toywt2.design <- svydesign(ids=~1, weights=~wts2, data=toy) # using ATE weights

adjout1.wt2 <- svyglm(out1.cost ~ treated, design=toywt2.design)
summary(adjout1.wt2); confint(adjout1.wt2)
```

### with TWANG ATT weights

```{r}
toywt3.design <- svydesign(ids=~1, 
                           weights=~get.weights(ps.toy, 
                                                stop.method = "es.mean"),
                           data=toy) # using twang ATT weights

adjout1.wt3 <- svyglm(out1.cost ~ treated, design=toywt3.design)
summary(adjout1.wt3); confint(adjout1.wt3)
```


## ... on Outcome 2 [a binary outcome]

For a binary outcome, we build the outcome model using the quasibinomial, rather than the usual binomial family. We use the same `svydesign` information as we built for outcome 1.

### Using ATT weights

```{r}
adjout2.wt1 <- svyglm(out2 ~ treated, design=toywt1.design, family=quasibinomial())
summary(adjout2.wt1)
exp(summary(adjout2.wt1)$coef)
exp(confint(adjout2.wt1))
```

### Using ATE weights

```{r}
adjout2.wt2 <- svyglm(out2.event ~ treated, design=toywt2.design, family=quasibinomial())
summary(adjout2.wt2)
exp(summary(adjout2.wt2)$coef)
exp(confint(adjout2.wt2))
```

### with TWANG ATT weights

```{r}
adjout2.wt3 <- svyglm(out2 ~ treated, design=toywt3.design,
                      family=quasibinomial())
summary(adjout2.wt3)
exp(summary(adjout2.wt3)$coef)
exp(confint(adjout2.wt3))
```

## ... on Outcome 3 [a time to event]

As before, subjects with `out2.event` = "Yes" are truly observed events, while those with `out2.event` == "No" are censored before an event can happen to them. 

### Using ATT weights

The Cox model comparing treated to control, weighting by ATT weights (`wts1`), is...

```{r}
adjout3.wt1 <- coxph(Surv(out3.time, out2) ~ treated, data=toy, weights=wts1)
summary(adjout3.wt1) 
```

The `exp(coef)` output gives the relative hazard of the event comparing treated subjects to control subjects.

And here's the check of the proportional hazards assumption...

```{r}
cox.zph(adjout3.wt1); plot(cox.zph(adjout3.wt1), var="treated")
```

### Using ATE weights

```{r}
adjout3.wt2 <- coxph(Surv(out3.time, out2) ~ treated, data=toy, weights=wts2)
summary(adjout3.wt2) 
```

And here's the check of the proportional hazards assumption...

```{r}
cox.zph(adjout3.wt2); plot(cox.zph(adjout3.wt2), var="treated")
```

### with TWANG ATT weights

```{r}
wts3 <- get.weights(ps.toy, stop.method = "es.mean")

adjout3.wt3 <- coxph(Surv(out3.time, out2) ~ treated, data=toy, weights=wts3)
summary(adjout3.wt3) 
```



## Results So Far (After Matching, Subclassification and Weighting)

Est. Treatment Effect (95\% CI) | Outcome 1 (Cost diff.) | Outcome 2 (Risk diff.) | Outcome 2 (Odds Ratio) | Outcome 3 (Relative Hazard Rate)
----------------: | -----------: | -----------: | -----------: | -----------: 
No covariate adjustment | **15.7** | **+0.11** | **1.56** | **0.86** 
(unadjusted, ATT) | (12.0, 19.3) | (-0.03, +0.25) | (0.87, 2.82) | (0.57, 1.29)
1:1 PS Match | **15.6** | **+0.11** | N/A | N/A
(`Match`: ATT) | (11.6, 19.6) | (-0.05, +0.25) | N/A | N/A
1:1 PS Match | **15.6** | N/A | **1.64** | **0.75**
("Regression", ATT) | (11.5, 19.6) | N/A | (0.77, 3.47) | (0.41, 1.38)
PS Subclassification | **7.9** | N/A | **1.15** | **0.79**
(ATE) | (4.1, 11.7) | N/A | (0.54, 2.44) | (0.49, 1.27)
ATT Weighting | **15.2** | N/A | **1.48** | **0.82**
(ATT) | (10.7, 19.8) | N/A | (0.74, 2.94) | (0.51, 1.32)
ATE Weighting | **7.1** | N/A | **1.29** | **0.83**
(ATE) | (0.1, 14.0) | N/A | (0.62, 2.67) | (0.63, 1.11)
`twang` ATT weights | **15.6** | N/A | **1.63** | **0.90**
(ATT) | (11.1, 20.0) | N/A | (0.84, 3.14) | (0.52, 1.55)

# Task 10. After direct adjustment for the linear PS, what is the estimated average  causal treatment effect?

## ... on Outcome 1 [a continuous outcome]

Here, we fit a linear regression model with `linps` added as a covariate.

```{r}
adj.reg.out1 <- lm(out1.cost ~ treated + linps, data=toy)
summary(adj.reg.out1); confint(adj.reg.out1) 
## provides treated effect and confidence interval estimates
```

## ... on Outcome 2 [a binary outcome]

Here, fit a logistic regression with `linps` added as a covariate

```{r}
adj.reg.out2 <- glm(out2 ~ treated + linps, data=toy, family=binomial())
summary(adj.reg.out2)
exp(coef(adj.reg.out2)) # produces odds ratio estimate
exp(confint(adj.reg.out2)) # produces 95% CI for odds ratio
```

## ... on Outcome 3 [a time-to-event outcome]

Again, subjects with `out2.event` No are right-censored, those with Yes for `out2.event` have their times to event observed.

We fit a Cox proportional hazards model predicting time to event (with event=Yes indicating non-censored cases) based on treatment group (treated) and now also the linear propensity score.

```{r}
adj.reg.out3 <- coxph(Surv(out3.time, out2) ~ treated + linps, data=toy)
summary(adj.reg.out3) 
```

The `exp(coef)` section indicates the relative hazard estimates and associated 95\% CI.

### Check proportional hazards assumption

Not the best of news here. The results are close to being statistically significant.

```{r}
cox.zph(adj.reg.out3)
plot(cox.zph(adj.reg.out3), var="treated")
plot(cox.zph(adj.reg.out3), var="linps")
```

## Results So Far (After Matching, Subclassification, Weighting, Adjustment)

Est. Treatment Effect (95\% CI) | Outcome 1 (Cost diff.) | Outcome 2 (Risk diff.) | Outcome 2 (Odds Ratio) | Outcome 3 (Relative Hazard Rate)
----------------: | -----------: | -----------: | -----------: | -----------: 
No covariate adjustment | **15.7** | **+0.11** | **1.56** | **0.86** 
(unadjusted, ATT) | (12.0, 19.3) | (-0.03, +0.25) | (0.87, 2.82) | (0.57, 1.29)
1:1 PS Match | **15.6** | **+0.11** | N/A | N/A
(`Match`: ATT) | (11.6, 19.6) | (-0.05, +0.25) | N/A | N/A
1:1 PS Match | **15.6** | N/A | **1.64** | **0.75**
("Regression", ATT) | (11.5, 19.6) | N/A | (0.77, 3.47) | (0.41, 1.38)
PS Subclassification | **7.9** | N/A | **1.15** | **0.79**
("Regression", ATE) | (4.1, 11.7) | N/A | (0.54, 2.44) | (0.49, 1.27)
ATT Weighting | **15.2** | N/A | **1.48** | **0.82**
(ATT) | (10.7, 19.8) | (0.74, 2.94) | (0.51, 1.32)
ATE Weighting | **7.1** | N/A | **1.29** | **0.83**
(ATE) | (0.1, 14.0) | N/A | (0.62, 2.67) | (0.63, 1.11)
`twang` ATT weights | **15.6** | N/A | **1.63** | **0.90**
(ATT) | (11.1, 20.0) | N/A | (0.84, 3.14) | (0.52, 1.55)
Direct Adjustment | **12.5** | N/A | **1.35** | **0.80**
(with `linps`, ATT) | (8.56, 16.36) | N/A | (0.71, 2.58) | (0.50, 1.26)

# Task 11. "Double Robust" Approach - Weighting + Adjustment, what is the estimated average causal effect of treatment?

This approach is essentially identical to the weighting analyses done in Task 9. The only change is to add `linps` to `treated` in the outcome models.

## ... on Outcome 1 [a continuous outcome]

### with ATT weights

The relevant regression approach uses the `svydesign` and `svyglm` functions from the `survey` package.

```{r}
toywt1.design <- svydesign(ids=~1, weights=~wts1, data=toy) # using ATT weights

dr.out1.wt1 <- svyglm(out1.cost ~ treated + linps, design=toywt1.design)
summary(dr.out1.wt1); confint(dr.out1.wt1)
```

### with ATE weights

```{r}
toywt2.design <- svydesign(ids=~1, weights=~wts2, data=toy) # using ATE weights

dr.out1.wt2 <- svyglm(out1.cost ~ treated + linps, design=toywt2.design)
summary(dr.out1.wt2); confint(dr.out1.wt2)
```

### with `twang` based ATT weights

```{r}
wts3 <- get.weights(ps.toy, stop.method = "es.mean")

toywt3.design <- svydesign(ids=~1, weights=~wts3, data=toy) # twang ATT weights

dr.out1.wt3 <- svyglm(out1.cost ~ treated + linps, design=toywt3.design)
summary(dr.out1.wt3); confint(dr.out1.wt3)
```


## ... on Outcome 2 [a binary outcome]

For a binary outcome, we build the outcome model using the quasibinomial, rather than the usual binomial family. We use the same `svydesign` information as we built for outcome 1.

### Using ATT weights

```{r}
dr.out2.wt1 <- svyglm(out2 ~ treated + linps, design=toywt1.design,
                      family=quasibinomial())
summary(dr.out2.wt1)
exp(summary(dr.out2.wt1)$coef)
exp(confint(dr.out2.wt1))
```

### Using ATE weights

```{r}
dr.out2.wt2 <- svyglm(out2.event ~ treated + linps, design=toywt2.design,
                      family=quasibinomial())
summary(dr.out2.wt2)
exp(summary(dr.out2.wt2)$coef)
exp(confint(dr.out2.wt2))
```

### Using `twang` ATT weights

```{r}
dr.out2.wt3 <- svyglm(out2 ~ treated + linps, design=toywt3.design,
                      family=quasibinomial())
summary(dr.out2.wt3)
exp(summary(dr.out2.wt3)$coef)
exp(confint(dr.out2.wt3))
```


## ... on Outcome 3 [a time to event]

As before, subjects with `out2.event` = "Yes" are truly observed events, while those with `out2.event` == "No" are censored before an event can happen to them. 

### Using ATT weights

The Cox model comparing treated to control, weighting by ATT weights (`wts1`), is...

```{r}
dr.out3.wt1 <- coxph(Surv(out3.time, out2) ~ treated + linps, data=toy, weights=wts1)
summary(dr.out3.wt1) 
```

The `exp(coef)` output gives the relative hazard of the event comparing treated subjects to control subjects.

And here's the check of the proportional hazards assumption...

```{r}
cox.zph(dr.out3.wt1); plot(cox.zph(dr.out3.wt1), var="treated")
```

### Using ATE weights

```{r}
dr.out3.wt2 <- coxph(Surv(out3.time, out2) ~ treated + linps, data=toy, weights=wts2)
summary(dr.out3.wt2) 
```

And here's the check of the proportional hazards assumption...

```{r}
cox.zph(dr.out3.wt2); plot(cox.zph(dr.out3.wt2), var="treated")
```

### Using `twang` ATT weights

```{r}
dr.out3.wt3 <- coxph(Surv(out3.time, out2) ~ treated + linps, 
                     data=toy, weights=wts3)
summary(dr.out3.wt3) 
```

The `exp(coef)` output gives the relative hazard of the event comparing treated subjects to control subjects.

And here's the check of the proportional hazards assumption...

```{r}
cox.zph(dr.out3.wt3); plot(cox.zph(dr.out3.wt3), var="treated")
```

# Task 12. Results

## Treatment Effect Estimates

We now can build the table of all of the outcome results we've obtained here.

Est. Treatment Effect (95\% CI) | Outcome 1 (Cost diff.) | Outcome 2 (Risk diff.) | Outcome 2 (Odds Ratio) | Outcome 3 (Relative Hazard Rate)
----------------: | -----------: | -----------: | -----------: | -----------: 
No covariate adjustment | **15.7** | **+0.11** | **1.56** | **0.86** 
(unadjusted, ATT) | (12.0, 19.3) | (-0.03, +0.25) | (0.87, 2.82) | (0.57, 1.29)
1:1 PS Match | **15.6** | **+0.11** | N/A | N/A
(`Match`: ATT) | (11.6, 19.6) | (-0.05, +0.25) | N/A | N/A
1:1 PS Match | **15.6** | N/A | **1.64** | **0.75**
("Regression", ATT) | (11.5, 19.6) | N/A | (0.77, 3.47) | (0.41, 1.38)
PS Subclassification | **7.9** | N/A | **1.15** | **0.79**
("Regression", ATE) | (4.1, 11.7) | N/A | (0.54, 2.44) | (0.49, 1.27)
ATT Weighting | **15.2** | N/A | **1.48** | **0.82**
(ATT) | (10.7, 19.8) | N/A | (0.74, 2.94) | (0.51, 1.32)
ATE Weighting | **7.1** | N/A | **1.29** | **0.83**
(ATE) | (0.1, 14.0) | N/A | (0.62, 2.67) | (0.63, 1.11)
`twang` ATT weights | **15.6** | N/A | **1.63** | **0.90**
(ATT) | (11.1, 20.0) | N/A | (0.84, 3.14) | (0.52, 1.55)
Direct Adjustment | **12.5** | N/A | **1.35** | **0.80**
(with `linps`, ATT) | (8.6, 16.4) | N/A | (0.71, 2.58) | (0.50, 1.26)
Double Robust     | **14.8** | N/A | **1.46** | **0.81**
(ATT wts + adj.)  | (10.6, 19.0) | N/A | (0.73, 2.94) | (0.50, 1.31)
Double Robust     | **6.7** | N/A | **1.28** | **0.83**
(ATE wts + adj.)  | (1.6, 11.7) | N/A | (0.61, 2.72) | (0.63, 1.11)
Double Robust     | **13.0** | N/A | **1.53** | **0.88**
(`twang` ATT wts + adj.)  | (8.5, 17.5) | N/A | (0.76, 3.07) | (0.50, 1.53)

So, for outcome 1, we have a significant result (indicating higher costs with the treatment) with every approach, and with outcomes 2 and 3, we do not.

## Quality of Balance: Standardized Differences and Variance Ratios

We're looking at the balance across the following 10 covariates and transformations here: `covA, covB, covC, covD, covE, covF[middle], covF[high], A squared, BxC` and `BxD` ...

Approach | Standardized Diffs | Variance Ratios
--------:| --: | --:
Most Desirable Values |	Between -10 and +10 |	Between 0.8 and 1.25
No Adjustments | -30 to 63 | 0.59 to 1.27
Subclassification Quintile 1 | -79 to 123 | 0 to 1.35
Quintile 2 | -54 to 47 | 0.40 to 2.99
Quintile 3 | -37 to 23 | 0.32 to 1.22
Quintile 4 | -64 to 32 | 0.84 to 1.85
Quintile 5 | 5 to 65 | 0.80 to 1.32
1:1 Propensity Matching | -13 to 20 | 0.62 to 1.23
Propensity Weighting, ATT | -6 to 13 | 0.64 to 1.20
Propensity Weighting, ATE | -14 to 19 | 0.86 to 1.12

## Quality of Balance: Rubin's Rules

Approach | Rubin 1 | Rubin 2 | Rubin 3
--: | --: | --: | --:
"Pass" Range, per Rubin |	0 to 50 |	0.5 to 2.0 | 0.5 to 2.0
No Adjustments	| 88	| 0.58	| 0.59 to 1.28
Subclassification: Quintile 1	| 61 |	0.48 |	0.02 to 1.32
Quintile 2 | 30 |	1.20 | 	0.36 to 3.19
Quintile 3 | 80 |	0.79 |	0.29 to 1.26
Quintile 4 | 28 | 0.80 |	0.83 to 1.91
Quintile 5 | 36 | 2.49 |	0.67 to 1.42
1:1 Propensity Matching |	37 |	1.42 |	0.56 to 1.28
Propensity Weighting, ATT |	6.2 |	1.20 |	Not evaluated
Propensity Weighting, ATE	| 5.0	| 0.90 |	Not evaluated

Clearly, the matching and propensity weighting show improvement over the initial (no adjustments) results, although neither is completely satisfactory in terms of all covariates. In practice, I would be comfortable with either a 1:1 match or a weighting approach, I think. It isn't likely that the subclassification will get us anywhere useful in terms of balance. Rubin's Rule 3 could also be applied after weighting on the propensity score.

# What is a Sensitivity Analysis for Matched Samples?

We'll study a formal sensitivity analysis approach for **matched** samples. Note well that this specific approach is appropriate only when we have 

1. a statistically significant conclusion 
2. from a matched samples analysis using the propensity score.

## Goal of a Formal Sensitivity Analysis for Matched Samples

To replace a general qualitative statement that applies in all observational studies, like ...

> the association we observe between treatment and outcome does not imply causation

or 

> hidden biases can explain observed associations

... with a quantitative statement that is specific to what is observed in a particular study, such as ...

> to explain the association seen in a particular study, one would need a hidden bias
of a particular magnitude.

If the association is strong, the hidden bias needed to explain it would be large.

- If a study is free of hidden bias (main example: a carefully randomized trial), this means that any two units (patients, subjects, whatever) that appear similar in terms of their observed covariates actually have the same chance of assignment to treatment.
- There is *hidden bias* if two units with the same observed covariates have different chances of receiving the treatment.

A **sensitivity analysis** asks: How would inferences about treatment effects be altered by hidden biases of various magnitudes?  How large would these differences have to be to alter the qualitative conclusions of the study?

The methods for building such sensitivity analyses are largely due to Paul Rosenbaum, and as a result the methods are sometimes referred to as **Rosenbaum bounds**.

## The Sensitivity Parameter, $\Gamma$

Suppose we have two units (subjects, patients), say, $j$ and $k$, with the same observed covariate values **x** but different probabilities $p$ of treatment assignment (possibly due to some unobserved covariate), so that **x**$_j$ = **x**$_k$ but that possibly $p_j \neq p_k$.

Units $j$ and $k$ might be *matched* to form a matched pair in our attempt to control overt bias due to the covariates **x**.

- The odds that units $j$ and $k$ receive the treatment are, respectively, $\frac{p_j}{1 - p_j}$ and $\frac{p_k}{1 - p_k}$, and the odds ratio is thus the ratio of these odds.

Imagine that we knew that this odds ratio for units with the same **x** was at most some number $\Gamma$, so that $\Gamma \geq 1$. That is,
 
$$
\frac{1}{\Gamma} \leq \frac{p_j(1 - p_j)}{p_k(1 - p_k)} \leq \Gamma
$$

We call $\Gamma$ the **sensitivity parameter**, and it is the basis for our sensitivity analyses.  

- If $\Gamma = 1$, then $p_j = p_k$ whenever **x**$_j$ = **x**$_k$, so the study would be free of hidden bias, and standard statistical methods designed for randomized trials would apply.

If $\Gamma = 2$, then two units who appear similar in that they have the same set of observed covariates **x**, could differ in their odds of receiving the treatment by as much as a factor of 2, so that one could be twice as likely as the other to receive the treatment.

So $\Gamma$ is a value between 1 and $\infty$ where the size of $\Gamma$ indicates the degree of a departure from a study free of hidden bias.

## Interpreting the Sensitivity Parameter, $\Gamma$

Again, $\Gamma$ is a measure of the degree of departure from a study that is free of hidden bias.  

A sensitivity analysis will consider possible values of $\Gamma$ and show how the inference for our outcomes might change under different levels of hidden bias, as indexed by $\Gamma$.  

- A study is *sensitive* if values of $\Gamma$ close to 1 could lead to inferences that are very different from those obtained assuming the study is free of hidden bias.
- A study is *insensitive* (a good thing here) if extreme values of $\Gamma$ are required to alter the inference.

When we perform this sort of sensitivity analysis, we will specify different levels of hidden bias (different $\Gamma$ values) and see how large a $\Gamma$ we can have while still retaining the fundamental conclusions of the matched outcomes analysis.

# Task 13. Sensitivity Analysis for Matched Samples, Outcome 1, using `rbounds`

In our matched sample analysis, for outcome 1 (cost) in the toy example, we saw a statistically significant result. A formal *sensitivity analysis* is called for, as a result, and we will accomplish one for this quantitative outcome, using the `rbounds` package.

The `rbounds` package is designed to work with the output from `Matching`, and can calculate Rosenbaum sensitivity bounds for the treatment effect, which help us understand the impact of hidden bias needed to invalidate our significant conclusions from the matched samples analysis.

## Rosenbaum Bounds for the Wilcoxon Signed Rank test (Quantitative outcome)

We have already used the Match function from the Matching package to develop a matched sample. Given this, we need only run the `psens` function from the `rbounds` package to obtain sensitivity results.

```{r}
X <- toy$linps ## matching on the linear propensity score
Tr <- as.logical(toy$treated)
Y <- toy$out1.cost
match1 <- Match(Tr=Tr, X=X, Y = Y, M = 1, replace=FALSE, ties=FALSE)
summary(match1)
psens(match1, Gamma = 5, GammaInc = 0.25)
```

If the study were free of hidden bias, that is, if $\Gamma = 1$, then there would be **strong** evidence that the treated patients had higher costs, and the specific Wilcoxon signed rank test we're looking at here shows a $p$ value < 0.0001. The sensitivity analysis we'll conduct now asks how this conclusion might be changed by hidden biases of various magnitudes, depending on the significance level we plan to use in our test.

## Specifying The Threshold $\Gamma$ value

From the output above, find the $\Gamma$ value where the upper bound for our $p$ value slips from "statistically significant" to "not significant" territory.

- We're doing a two-tailed test, with a 95\% confidence level, so the $\Gamma$ statistic for this situation is between 3.50 and 3.75, since that is the point where the upper bound for the *p* value crosses the threshold of $\alpha/2 = 0.025$.

So this study's conclusion (that treated patients had significantly higher costs) would still hold even in the face of a hidden bias with $\Gamma = 3.5$, but not with $\Gamma = 3.75$.

The tipping point for the sensitivity parameter is a little over 3.5. To explain away the observed association between treatment and this outcome (cost), a hidden bias or unobserved covariate would need to increase the odds of treatment by more than a factor of $\Gamma = 3.5$. 

Returning to the output:

- If instead we were doing a one-tailed test, with a 95\% confidence level, then the $\Gamma$ statistic for this situation is between 4 and 4.25, since that is the point where the upper bound for the *p* value crosses $\alpha = 0.05$.
- And if instead we were doing a one-tailed test with a 90\% confidence level, then the $\Gamma$ statistic would be between 4.75 and 5.0, since that is where the upper bound for the *p* value crosses $\alpha = 0.10$.

## Interpreting $\Gamma$ appropriately

$\Gamma$ tells you only *how big a bias is needed to change the answer*. By itself, it says NOTHING about the likelihood that a bias of that size is present in your study, except that, of course, smaller biases hide more effectively than large ones, on average.

- In some settings, we'll think of $\Gamma$ in terms of small (< 1.5), modest (1.5 - 2.5), moderate (2.5 - 4) and large (> 4) hidden bias requirements. But these are completely arbitrary distinctions, and I can provide no good argument for their use.

The **only** defense against hidden bias affecting your conclusions is to try to reduce the potential for hidden bias in the first place. We work on this via careful desigh of  observational studies, especially by including as many different dimensions of the selection problem as possible in your propensity model.

## An Alternate Approach - the Hodges-Lehman estimate

```{r}
hlsens(match1)
```

If the $\Gamma$ value is 3.0, then this implies that the Hodges-Lehmann estimate might be as low as 7.5 or as high as 24.1 (it is 16.5 in the absence of hidden bias in this case - when $\Gamma$ = 0.)

## What about other types of outcomes?

The `rbounds` package can evaluate binary outcomes using the `binarysens` and `Fishersens` functions.

Survival outcomes can be assessed, too, but not, I believe, using `rbounds` unless there is no censoring. Some time back, I built a spreadsheet for this task, which I'll be happy to share.

## What about when we match 1:2 or 1:3 instead of 1:1?

The `mcontrol` function in the `rbounds` package can be helpful in such a setting.

# Wrapup

If you run this script, you'll wind up with a version of the `toy` tibble that contains 200 observations on 28 variables, along with a `codebook` list.

You'll also have two new functions, called `szd` and `rubin3`, that, with some modification, may be useful elsewhere.

To drop everything else in the global environment created by this Markdown file, run the code that follows.

```{r clean up}
rm(list = c("adj.m.out1", "adj.m.out2", "adj.m.out3", 
            "adj.reg.out1", "adj.reg.out2", "adj.reg.out3", 
            "adj.s.out3", "adjout1.wt1", "adjout1.wt2", 
            "adjout2.wt1", "adjout2.wt2", "adjout3.wt1", 
            "adjout3.wt2", "bal.after.wts1", "bal.after.wts2", 
            "bal.before.wts1", "bal.before.wts2", "bal.wts1",
            "bal.wts2", "balance.ate.weights", "balance.att.weights",
            "cov.sub", "covlist", "covnames", "covs", 
            "d.all", "d.q1", "d.q2", "d.q3", "d.q4", "d.q5", 
            "dr.out1.wt1", "dr.out1.wt2", "dr.out2.wt1",
            "dr.out2.wt2", "dr.out3.wt1", "dr.out3.wt2", "est.st", 
            "factorlist", "high", "i", "low", "match1", "matches", 
            "mb1", "mixedmodel.out1", "post.szd", "post.vratio", 
            "pre.szd", "pre.vratio", "psmodel", "quin1", 
            "quin1.out1", "quin1.out2", "quin2", "quin2.out1", 
            "quin2.out2", "quin3", "quin3.out1", "quin3.out2", 
            "quin4", "quin4.out1", "quin4.out2", "quin5", 
            "quin5.out1", "quin5.out2", "rubin1.match", "rubin1.q1", 
            "rubin1.q2", "rubin1.q3", "rubin1.q4", "rubin1.q5", 
            "rubin1.sub", "rubin1.unadj", "rubin2.match", 
            "rubin2.q1", "rubin2.q2", "rubin2.q3", "rubin2.q4", 
            "rubin2.q5", "rubin2.sub", "rubin2.unadj", "rubin3.both", 
            "rubin3.matched", "rubin3.q1", "rubin3.q2", "rubin3.q3", 
            "rubin3.q4", "rubin3.q5", "rubin3.unadj", "se.q1", 
            "se.q2", "se.q3", "se.q4", "se.q5", "se.st", "temp", 
            "temp.result1", "temp.result2", "tempsort", 
            "toy.matchedsample", "toy.rubin3", "toy.szd", 
            "toy_df", "toywt1.design", "toywt2.design", "Tr", 
            "unadj.out1", "unadj.out2", "unadj.out3", "varlist", "X", "Y"))
```

